

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/myfavicon.png">
  <link rel="icon" href="/img/myfavicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="EverNorif">
  <meta name="keywords" content="">
  
    <meta name="description" content="本文首先回顾了机器学习的流程，并提出问题，机器学习为什么可行。之后分别分析了在有限容量和无限容量假设空间上，机器学习的可行性。在推导过程中，引出成长函数、Shatter、Break Point等概念，最终得到著名的VC不等式，以及VC Dimension，得到机器学习的泛化边界。最后在完成可行性分析之后，讨论了这一过程带给我们的启示与意义。">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习基础(2)-机器学习可行性">
<meta property="og:url" content="https://evernorif.github.io/2023/08/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%AF%E8%A1%8C%E6%80%A7/index.html">
<meta property="og:site_name" content="EverNorif">
<meta property="og:description" content="本文首先回顾了机器学习的流程，并提出问题，机器学习为什么可行。之后分别分析了在有限容量和无限容量假设空间上，机器学习的可行性。在推导过程中，引出成长函数、Shatter、Break Point等概念，最终得到著名的VC不等式，以及VC Dimension，得到机器学习的泛化边界。最后在完成可行性分析之后，讨论了这一过程带给我们的启示与意义。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://evernorif.github.io/2023/08/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%AF%E8%A1%8C%E6%80%A7/%E4%BB%8EN-1%E4%B8%AA%E6%95%B0%E6%8D%AE%E5%BE%97%E5%88%B0N%E4%B8%AA%E6%95%B0%E6%8D%AE.png">
<meta property="og:image" content="https://evernorif.github.io/2023/08/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%AF%E8%A1%8C%E6%80%A7/%E8%AF%AF%E5%B7%AE%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%A4%8D%E6%9D%82%E5%BA%A6.png">
<meta property="article:published_time" content="2023-08-18T12:30:13.000Z">
<meta property="article:modified_time" content="2023-08-22T12:40:41.677Z">
<meta property="article:author" content="EverNorif">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://evernorif.github.io/2023/08/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%AF%E8%A1%8C%E6%80%A7/%E4%BB%8EN-1%E4%B8%AA%E6%95%B0%E6%8D%AE%E5%BE%97%E5%88%B0N%E4%B8%AA%E6%95%B0%E6%8D%AE.png">
  
  
  
  <title>机器学习基础(2)-机器学习可行性 - EverNorif</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  



  
<link rel="stylesheet" href="/css/macpanel.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"evernorif.github.io","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/bilibiliTV.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"gtag":null},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 80vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>EverNorif</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tools/" target="_self">
                <i class="iconfont icon-briefcase"></i>
                <span>工具</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button"
                 data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                <i class="iconfont icon-bilibili"></i>
                <span>番剧</span>
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                
                  
                  
                  
                  <a class="dropdown-item" href="/bangumis/" target="_self">
                    <i class="iconfont icon-bilibili-fill"></i>
                    <span>追番</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/cinemas/" target="_self">
                    <i class="iconfont icon-youtube-fill"></i>
                    <span>追剧</span>
                  </a>
                
              </div>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/post.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="机器学习基础(2)-机器学习可行性"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-08-18 20:30" pubdate>
          2023年8月18日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          6k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          <!-- compatible with older versions-->
          51 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
        <div class="scroll-down-bar">
          <i class="iconfont icon-arrowdown"></i>
        </div>
      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">机器学习基础(2)-机器学习可行性</h1>
            
              <p id="updated-time" class="note note-info" style="display: none">
                
                  
                    <!-- compatible with older versions-->
                    本文最后更新于：2023-08-22T20:40:41+08:00
                  
                  

                
              </p>
            
            
              <div class="markdown-body">
                
                <h1 id="问题描述">问题描述</h1>
<p>首先我们回顾机器学习的整个流程。首先我们认为存在一个真实的函数<span
class="math inline">\(f:X \rightarrow
Y\)</span>，同时我们存在现有的数据（训练集）<span
class="math inline">\(D={(x_1,y_1),...,(x_N,y_N)}\)</span>。完成机器学习，我们需要有一个假设空间<span
class="math inline">\(H\)</span>，即机器学习的模型，另外还需要有一个Learning
Algorithm <span
class="math inline">\(A\)</span>，即机器学习的策略和算法。通过学习算法，从假设空间中选择一个较好的结果映射<span
class="math inline">\(g:X \rightarrow Y\)</span>，使得<span
class="math inline">\(g\)</span>和<span
class="math inline">\(f\)</span>尽可能接近。</p>
<p>更具体地说，在选择<span
class="math inline">\(g\)</span>的过程中，我们会选择经验误差（概论中提到的经验风险或者经验损失）尽可能地小的模型，这里将经验风险记作<span
class="math inline">\(E_{in}(g;D)\)</span>，其中<span
class="math inline">\(D\)</span>表示训练集。另一方面，我们也希望这个模型能够在没有看过的数据上得到很好的表现，也就是说泛化误差（概论中提到的期望风险）尽可能地小，这里将泛化风险记作<span
class="math inline">\(E_{out}(g;X)\)</span>，其中<span
class="math inline">\(X\)</span>是所有可能的输入，也就是输入空间。简单来说，如果机器学习顺利进行，我们会从假设空间<span
class="math inline">\(H\)</span>中选择一个<span
class="math inline">\(E_{in}\)</span>最小的模型，而这个模型能够达到的效果是它在没有看过的数据上也能得到很好的表现，也就是<span
class="math inline">\(E_{out}\)</span>也很小。</p>
<p>那么现在我们提出的问题是，为什么机器学习能够做到这一点，也就是机器学习的可行性。进一步，机器学习的可行性就是要解释下面两个问题：</p>
<ul>
<li>为什么<span class="math inline">\(E_{in} \approx
E_{out}\)</span>，也就是为什么模型能够从训练集向外推广到没有见过的数据上</li>
<li>为什么<span class="math inline">\(E_{out} \approx
0\)</span>，也就是模型的效果怎么能比较好</li>
</ul>
<p>在下面的分析中，我们以二分类为例讨论机器学习的可行性。在二分类的情况下，我们的输出空间为<span
class="math inline">\(Y=\{+1,-1\}\)</span>，同时经验误差<span
class="math inline">\(E_{in}\)</span>和泛化误差<span
class="math inline">\(E_{out}\)</span>计算如下，它们都是关于映射<span
class="math inline">\(h\)</span>的函数，其中<span
class="math inline">\(h\in H\)</span>。 <span class="math display">\[
E_{in} (h) = \frac{1}{N}\sum_{i=1}^{N} I[h(x_i) \neq f(x_i)]
\\
E_{out}(h) = P\{ h(x) \neq f(x) \}
\]</span> 这里的<span
class="math inline">\(I(x)\)</span>是指示函数，如果<span
class="math inline">\(x\)</span>为真则返回1，如果<span
class="math inline">\(x\)</span>为假则返回0。</p>
<h1 id="有限容量的假设空间">有限容量的假设空间</h1>
<p>首先我们考虑对于<strong>一个固定的</strong><span
class="math inline">\(h\in H\)</span>，我们能够得出什么样的结论。</p>
<p>由于<span
class="math inline">\(E_{in}\)</span>的计算是在训练集上的，<span
class="math inline">\(E_{out}\)</span>的计算是在模型没有看过的数据上的，但是这两部分数据是独立同分布的，对应真实存在的一个数据分布<span
class="math inline">\(P\)</span>，因此根据Hoeffding不等式，我们可以得到下面的结论：
<span class="math display">\[
P[|E_{in}(h)-E_{out}(h)| \ge \epsilon] \le 2\exp(-2\epsilon^2N)
\]</span></p>
<blockquote>
<p>Hoeffding不等式描述如下：</p>
<p>假设<span
class="math inline">\(X_1,X_2,...,X_N\)</span>是独立随机变量，且<span
class="math inline">\(X_i \in
[a_i,b_i],i=1,2,...,N\)</span>；那么对任意的<span
class="math inline">\(t&gt;0\)</span>，有下面的不等式成立： <span
class="math display">\[
P[|\bar{X} -E(\bar{X})| \ge t] \le
\exp[-\frac{2N^2t^2}{\sum_{i=1}^{N}(b_i-a_i)^2}]
\]</span> 考虑联系到这里的<span
class="math inline">\(E_{in}\)</span>和<span
class="math inline">\(E_{out}\)</span>，则分别可以对应到<span
class="math inline">\(\bar{X},E(\bar{X})\)</span>。同时这里的<span
class="math inline">\(X_i \in[0,1]\)</span>，所以有上面的结论。</p>
</blockquote>
<p>上面的式子含义是，对于一个固定的<span class="math inline">\(h\in
H\)</span>来说，坏事发生的机率是比较小的。这里的坏事指的是<span
class="math inline">\(|E_{in}(h)-E_{out}(h)| \ge
\epsilon\)</span>，也就是做不到<span class="math inline">\(E_{in}
\approx E_{out}\)</span>。</p>
<p>一般来说，如果<span class="math inline">\(h\)</span>是固定的，当<span
class="math inline">\(N\)</span>很大的时候，我们能够认为<span
class="math inline">\(E_{in} \approx
E_{out}\)</span>。当然这只能保证模型能够外推，并不能保证模型的效果，也就是不能保证<span
class="math inline">\(E_{out} \approx 0\)</span>。这是因为我们的<span
class="math inline">\(h\)</span>是固定的，不能保证这个<span
class="math inline">\(h\)</span>在训练集上的<span
class="math inline">\(E_{in} \approx 0\)</span>。在这种情况下，如果<span
class="math inline">\(h\)</span>在训练集上的效果很糟糕，那么它的确能够外推到没有见过的数据上，只不过表现也是很糟糕。因此，机器学习采用的方法是给定一个假设空间<span
class="math inline">\(H\)</span>，希望能够在这个假设空间中找到一个<span
class="math inline">\(h\)</span>，使得<span class="math inline">\(E_{in}
\approx 0\)</span>。</p>
<p>我们记假设空间的大小<span
class="math inline">\(|H|=M\)</span>，那么<span
class="math inline">\(M\)</span>可能是有限的，可能是无限的。我们首先考虑有限容量的假设空间。</p>
<p>在有限容量的假设空间<span class="math inline">\(H =
\{h_1,h_2,...,h_M\}\)</span>中，学习算法从中选择一个<span
class="math inline">\(h\)</span>作为输出映射。我们来考虑此时坏事发生的概率有多少：
<span class="math display">\[
\begin{aligned}
P[\exists h \in H,s.t. &amp;|E_{in}(h)-E_{out}(h)| \ge \epsilon] \\
&amp; \le \sum_{i=1}^{M} P[|E_{in}(h_i)-E_{out}(h_i)| \ge \epsilon] \\
&amp; \le \sum_{i=1}^{M} 2\exp(-2\epsilon^2N) = 2M\exp(-2\epsilon^2N)
\end{aligned}
\]</span> 上面的推导很容易理解。因为学习算法选择的<span
class="math inline">\(h\)</span>也是<span
class="math inline">\(h1,h2,...,h_M\)</span>中的一员，它发生坏事的概率一定会小于所有<span
class="math inline">\(h_i\)</span>发生坏事的概率之和（Union
Bound）。</p>
<p>考虑上面的不等式，当样本容量<span
class="math inline">\(N\)</span>足够大的时候，坏事发生的概率是比较小的。也就是说面对<span
class="math inline">\(M\)</span>个（有限个）假设函数，学习算法<span
class="math inline">\(A\)</span>可以从中选择一个<span
class="math inline">\(E_{in}\)</span>相对较小的假设函数作为最终的结果映射，并且能够满足<span
class="math inline">\(E_{in}\approx E_{out}\)</span>。同时由于<span
class="math inline">\(E_{in}\)</span>相对较小，所以<span
class="math inline">\(E_{out}\)</span>也相对较小。也就解决了我们在问题描述中提到的两个问题。</p>
<p>至此，我们已经证明了在有限容量的假设空间上，机器学习的可行性。下一步，我们需要证明在无限容量的假设空间上，机器学习的可行性。</p>
<h1 id="无限容量的假设空间">无限容量的假设空间</h1>
<p>通过上面的描述，我们直到了当<span
class="math inline">\(M\)</span>有限，机器学习是可行的。但是在实际上，我们通常面对的都是无限容量的假设空间。当<span
class="math inline">\(M\)</span>变得无限大，上面的不等式显然也没有什么意义。不过我们可以考虑上面的推导过程中，<span
class="math inline">\(M\)</span>是通过Union
Bound进行引入的，我们考虑一个<span
class="math inline">\(h\)</span>发生坏事的概率小于等于所有<span
class="math inline">\(h_i\)</span>发生坏事的概率之和。这样的放缩是合理的，但是这也许是一个过度的估计（over-estimating）。如果考虑概率之和，那也就是认为每个<span
class="math inline">\(h_i\)</span>发生坏事的情况相互独立互不相关，但是实际上很大部分的<span
class="math inline">\(h_i\)</span>，它们发生坏事的情况是互相重叠的。因此在考虑无限容量的假设空间时，我们需要想办法将这个过度估计进行修正。</p>
<p>首先我们引入相关概念Shatter、成长函数、Break Point、Bounding
Function，为后续的证明进行铺垫。</p>
<h2 id="成长函数shatterbreak-point">成长函数、Shatter、Break Point</h2>
<p>我们面对的是无限容量的假设空间<span
class="math inline">\(H\)</span>。既然无限容量较难处理，一种想法是能不能将无限的假设空间划分成有限中假设函数类别，而属于同一类别的假设函数，发生坏事的情况是重叠的。</p>
<p>考虑一个数据集<span
class="math inline">\(D=\{x_1,x_2,...,x_N\}\)</span>，对于一个给定的假设函数<span
class="math inline">\(h\)</span>，它对<span
class="math inline">\(D\)</span>的每一个数据<span
class="math inline">\(x_i\)</span>只能做出+1或者-1的判断。换句话说，在一个给定的数据集<span
class="math inline">\(D\)</span>看来，以假设函数的预测结果<span
class="math inline">\(h(x_i)\)</span>为导向，假设空间中的函数可以分为不同的类别，并且这个类别的个数是有限个。很容易得到类别个数最高不能超过<span
class="math inline">\(2^N\)</span>。当然也会存在达不到<span
class="math inline">\(2^N\)</span>的情况，这是由于对于一些假设空间，某些结果是达不到的，例如对于感知机算法，一些线性不可分的结果是达不到的。</p>
<p>于是，对于一个给定的假设空间<span
class="math inline">\(H\)</span>，以及一个给定的数据<span
class="math inline">\(D=\{x_1,x_2,...,x_N\},x_i\in X\)</span>
，上面的类别数记为<span
class="math inline">\(H(x_1,x_2,...,x_N)\)</span>，它表示<span
class="math inline">\(H\)</span>在<span
class="math inline">\(D\)</span>上能够产生的所有二分类的结果数。很明显地，有<span
class="math inline">\(H(x_1,x_2,...,x_N) \le 2^N\)</span>。</p>
<p>上面的结果数<span
class="math inline">\(H(x_1,x_2,...,x_N)\)</span>，与假设空间<span
class="math inline">\(H\)</span>以及给定数据集<span
class="math inline">\(D\)</span>有关。我们希望排除数据集<span
class="math inline">\(D\)</span>的影响，则引入一个<strong>成长函数</strong>的概念。成长函数记为<span
class="math inline">\(m_H(N)\)</span>，定义如下： <span
class="math display">\[
m_H(N) = max_{\{x_1,x_2,...,x_N\}} [H(x_1,x_2,...,x_N)]
\]</span> 成长函数<span
class="math inline">\(m_H(N)\)</span>表示对于一个特定的假设空间<span
class="math inline">\(H\)</span>，它在所有大小为N的数据集上能够产生的二分类结果数，其中最大的那一个数量。可以看到，成长函数<span
class="math inline">\(m_H(N)\)</span>在给定假设空间<span
class="math inline">\(H\)</span>的情况下，仅与数据集<span
class="math inline">\(D\)</span>的大小有关。</p>
<p>而<strong>Shatter</strong>的概念可以描述如下：对于一个给定的假设空间<span
class="math inline">\(H\)</span>来说，如果存在一个数据集<span
class="math inline">\(D=\{x_1,x_2,...,x_N\}，x_i\in
X\)</span>，使得<span class="math inline">\(H(x_1,x_2,...,x_N) =
2^N\)</span>，则称假设空间<span
class="math inline">\(H\)</span>能够Shatter（打散）这个数据集<span
class="math inline">\(D\)</span>。而如果<span
class="math inline">\(H\)</span>不能Shatter <span
class="math inline">\(D\)</span>，它的含义是对于任意的<span
class="math inline">\(D = \{x_1,x_2,...,x_N\} ，x_i\in
X\)</span>，假设空间<span
class="math inline">\(H\)</span>都无法产生所有的二分类结果。我们可以得到，如果<span
class="math inline">\(H\)</span> 能够Shatter <span
class="math inline">\(D,(|D| =
N，数据集D的大小为N)\)</span>，则表示<span class="math inline">\(m_H(N)
= 2^N\)</span>，否则，则有<span class="math inline">\(m_H(N) &lt;
2^N\)</span>。</p>
<p>根据Shatter的概念，我们可以知道：对于一个给定的假设空间<span
class="math inline">\(H\)</span>，假如它不能Shatter大小为k的数据集<span
class="math inline">\(D\)</span>，那么对于任意<span
class="math inline">\(k&#39;&gt;k\)</span>，<span
class="math inline">\(H\)</span>也无法Shatter大小为<span
class="math inline">\(k&#39;\)</span>的数据集<span
class="math inline">\(D&#39;\)</span>。换成使用成长函数<span
class="math inline">\(m_H(N)\)</span>的说法，则是如果存在<span
class="math inline">\(m_H(k) &lt; 2^k\)</span>，那么对应任意的<span
class="math inline">\(k&#39;&gt;k\)</span>，也有<span
class="math inline">\(m_H(k&#39;)&lt;2^{k&#39;}\)</span>。<strong>Break
Point</strong>则是指这样一个数k，它是第一个不能被<span
class="math inline">\(H\)</span>
Shatter的数据集的大小，也就是说，它是满足<span
class="math inline">\(m_H(k) \neq
2^k\)</span>的最小值。它的意义可以解释如下：当Break Point的值为<span
class="math inline">\(k\)</span>时，我们在任选<span
class="math inline">\(k\)</span>个数据<span
class="math inline">\(x_i\)</span>，我们无法得到全部的二分类结果（<span
class="math inline">\(2^k\)</span>个结果）。</p>
<p>Break Point是假设空间<span
class="math inline">\(H\)</span>的一个伴随性质，如果我们确定了假设空间<span
class="math inline">\(H\)</span>，那么对应的Break
Point也就能够随之确定。当然存在一种情况是Break
Point不存在，那也就是说对于这样的假设空间<span
class="math inline">\(H\)</span>，它对任意的<span
class="math inline">\(N\)</span>都有<span class="math inline">\(m_H(N) =
2^N\)</span>。</p>
<h2 id="bounding-function">Bounding Function</h2>
<p>假设对于一个假设空间<span class="math inline">\(H\)</span>，它的Break
Point值为<span
class="math inline">\(k\)</span>。那么我们可以发现，当<span
class="math inline">\(N&gt;k\)</span>的时候，Break
Point限制了成长函数<span
class="math inline">\(m_H(N)\)</span>的大小。综合来说，影响成长函数<span
class="math inline">\(m_H(N)\)</span>的因素主要有两个：</p>
<ul>
<li>抽样数据集的大小<span class="math inline">\(N\)</span></li>
<li>Break Point 的值 <span
class="math inline">\(k\)</span>，这个值伴随假设空间<span
class="math inline">\(H\)</span>的确定而确定</li>
</ul>
<p>我们可以引入一个新的函数，称为Bounding Function，记为<span
class="math inline">\(B(N,k)\)</span>。它表示当Break Point值为<span
class="math inline">\(k\)</span>的时候，成长函数<span
class="math inline">\(m_H(N)\)</span>可能的最大值。也就是说，<span
class="math inline">\(B(N,k)\)</span>是<span
class="math inline">\(m_H(N)\)</span>的上界，有： <span
class="math display">\[
m_H(N) \le B(N,k)
\]</span> 此时对于这个假设空间H，它的Break Point值为<span
class="math inline">\(k\)</span>。</p>
<p>接下来考虑这个Bounding Function的范围。</p>
<p>当<span class="math inline">\(k=1\)</span>的时候，有<span
class="math inline">\(B(N,k)=1\)</span>。这是因为对于大小为N的数据集<span
class="math inline">\({x_1,x_2,...,x_N}\)</span>，无论结果如何，我们只能定义一种对应的输出。如果我们想要在这个基础上增加下一种对应的输出，就会与<span
class="math inline">\(k=1\)</span>冲突。因为<span
class="math inline">\(k=1\)</span>的含义是任选1个数据，我们无法得到全部的二分类结果，也就是对于一个<span
class="math inline">\(x_i\)</span>，我们的分类结果只能有一个。</p>
<p>当<span class="math inline">\(N&lt;k\)</span>的时候，有<span
class="math inline">\(m_H(N) = 2^N\)</span>，所以<span
class="math inline">\(B(N,k)=2^N\)</span>。</p>
<p>当<span class="math inline">\(N=k\)</span>的时候，由于<span
class="math inline">\(k\)</span>是第一个不能被Shatter的大小，所以能够得到的二分类结果至少要比全部的二分类结果少一个，即<span
class="math inline">\(B(N,k) = 2^N-1\)</span>。</p>
<p>结合上面三种情况，有： <span class="math display">\[
B(N,k) = \begin{cases}
1, \quad k=1\\
2^N, \quad N&lt;k \\
2^N-1, \quad N=k
\end{cases}
\]</span> 接下来考虑<span
class="math inline">\(N&gt;k\)</span>的情况。</p>
<p>我们可以在<span
class="math inline">\(N-1\)</span>个数据的基础上考虑<span
class="math inline">\(N\)</span>个数据的由来。<span
class="math inline">\(N\)</span>个数据就可以看作是在<span
class="math inline">\(N-1\)</span>个数据上增加一个数据。即在数据集<span
class="math inline">\(\{x_1,x_2,...,x_{N-1}\}\)</span>的基础上增加一个数据<span
class="math inline">\(x_N\)</span>。从而<span
class="math inline">\(N\)</span>个数据对应的二分类结果也可以从<span
class="math inline">\(N-1\)</span>个数据对应的二分类结果上得来。对于一个特定的二分类结果<span
class="math inline">\(\{h(x_1),h(x_2),...,h(x_{N-1})\}\)</span>，在新判断第<span
class="math inline">\(N\)</span>个数据的时候，有两种情况：</p>
<ul>
<li>第一种是<span
class="math inline">\(h(x_N)\)</span>可以是+1也可以是-1，都可以选择。这种情况出现的个数记为<span
class="math inline">\(\alpha\)</span></li>
<li>第二种是<span
class="math inline">\(h(x_N)\)</span>只能选择+1或者-1的其中一种。这种情况出现的个数记为<span
class="math inline">\(\beta\)</span></li>
</ul>
<img src="/2023/08/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%AF%E8%A1%8C%E6%80%A7/%E4%BB%8EN-1%E4%B8%AA%E6%95%B0%E6%8D%AE%E5%BE%97%E5%88%B0N%E4%B8%AA%E6%95%B0%E6%8D%AE.png" srcset="/img/bilibiliTV.gif" lazyload class="" title="从N-1个数据得到N个数据">
<p>由此，我们可以说<span class="math inline">\(B(N,k) = 2\alpha +
\beta\)</span>。</p>
<p>在上面的<span
class="math inline">\(N\)</span>个点构成的二分类结果集中，由于<span
class="math inline">\(N&gt;k\)</span>，所以应该满足<span
class="math inline">\(H\)</span>无法Shatter 大小为<span
class="math inline">\(k\)</span>的数据集。所以在<span
class="math inline">\(\{x_1,x_2,...,x_{N-1},x_N\}\)</span>任选<span
class="math inline">\(k\)</span>个<span
class="math inline">\(x_i\)</span>，结果集中不能包括所有的二分类结果。因此，在<span
class="math inline">\(\{x_1,x_2,...,x_{N-1}\}\)</span>中任取<span
class="math inline">\(k\)</span>个点，应该满足结果集中不能包括所有的二分类结果，否则就和上面无法Shatter
大小为<span class="math inline">\(k\)</span>的数据集相悖。而<span
class="math inline">\(\{x_1,x_2,...,x_{N-1}\}\)</span>的所有二分类结果大小为<span
class="math inline">\(\alpha+\beta\)</span>，所以有<span
class="math inline">\(\alpha + \beta \le B(N-1,k)\)</span>。</p>
<p>同时考虑情况1中的<span
class="math inline">\(\{x_1,x_2,...,x_{N-1}\}\)</span>，如果在其中任取<span
class="math inline">\(k-1\)</span>个点，应该满足结果集中不能包括所有的二分类结果，否则的话，我们可以增加对应的<span
class="math inline">\(x_N\)</span>，<span
class="math inline">\(x_N\)</span>的结果既可以是+1又可以是-1，这样就违反了<span
class="math inline">\(H\)</span>无法Shatter 大小为<span
class="math inline">\(k\)</span>的数据集。这一部分的二分类结果集大小为<span
class="math inline">\(\alpha\)</span>，所以有<span
class="math inline">\(\alpha \le B(N-1,k-1)\)</span>。</p>
<p>所以有，当<span class="math inline">\(N&gt;k\)</span>时，有<span
class="math inline">\(B(N,k) = 2\alpha + \beta = \alpha + \alpha + \beta
\le B(N-1,k) + B(N-1,k-1)\)</span>。在<span class="math inline">\(N \le
k\)</span>的情况下，<span
class="math inline">\(B(N,k)\)</span>的值在上面已经提供，经过计算，发现也能满足这个不等式（确切地说在<span
class="math inline">\(N\le
k\)</span>的情况下，能够直接满足等式）。因此对于所有的<span
class="math inline">\(N\)</span>和<span
class="math inline">\(k\)</span>，我们都有： <span
class="math display">\[
B(N,k)  \le B(N-1,k) + B(N-1,k-1)
\]</span> 而根据这个不等式，我们可以得到下面的不等式： <span
class="math display">\[
B(N,k) \le \sum_{i=0}^{k-1}\binom{N}{i}
\]</span> 这个不等式可以通过数学归纳法来进行证明：</p>
<p>1.当<span class="math inline">\(N=1\)</span>的时候，有： <span
class="math display">\[
\begin{cases}
B(1,1) = 1 \le \sum_{i=0}^{0} \binom{1}{i} = 1, \quad k=1\\
B(1,k) = 2 \le \sum_{i=0}^{k-1} \binom{1}{i} = 1+1 = 2, \quad k &gt;1
\end{cases}
\]</span> 2.当<span class="math inline">\(N=2\)</span>的时候，有： <span
class="math display">\[
\begin{cases}
B(2,1) = 1 \le \sum_{i=0}^{0} \binom{2}{i} = 1, \quad k=1\\
B(2,2) = 3 \le \sum_{i=0}^{1} \binom{2}{i} = 1+2 = 3, \quad k=2\\
B(2,k) = 4 \le \sum_{i=0}^{k} \binom{2}{i} = 1+2+1 = 4, \quad k&gt;2
\end{cases}
\]</span> 3.假设对于任何的<span
class="math inline">\(N&lt;N_0\)</span>，都有结论成立，则当<span
class="math inline">\(N=N_0\)</span>的时候，有： <span
class="math display">\[
\begin{aligned}
B(N,k)
&amp;\le B(N-1,k) + B(N-1, k-1)\\
&amp;\le \sum_{i=0}^{k-1} \binom{N-1}{i} + \sum_{i=0}^{k-2}
\binom{N-1}{i} \\
&amp;= 1 + \sum_{i=1}^{k-1} \binom{N-1}{i} + \sum_{i=1}^{k-1}
\binom{N-1}{i-1}\\
&amp;= 1 + \sum_{i=1}^{k-1} [\binom{N-1}{i} + \binom{N-1}{i-1}]\\
&amp;= 1 + \sum_{i=1}^{k-1} \binom{N}{i}\\
&amp;= \sum_{i=0}^{k-1} \binom{N}{i}
\end{aligned}
\]</span> 所以我们可以得到如下的结论，如果对于一个假设空间<span
class="math inline">\(H\)</span>来说，Break Point存在且值为<span
class="math inline">\(k\)</span>的话，有如下不等式成立：（如果没有Break
Point ，则表示<span class="math inline">\(k = \infty\)</span>） <span
class="math display">\[
m_H(N) \le B(N,k) \le \sum_{i=0}^{k-1} \binom{N}{i}
\]</span></p>
<h2 id="不等式代换">不等式代换</h2>
<p>回到我们的目标，我们希望能够在当<span
class="math inline">\(M\)</span>为无限的时候证明： <span
class="math display">\[
P[\exists h \in H, s.t. |E_{in}(h) - E_{out}(h)| \ge \epsilon ] \le
某个系数
\]</span> 其中这个系数的变化由<span
class="math inline">\(N\)</span>主导。例如在当<span
class="math inline">\(M\)</span>有限的时候，这个系数就是<span
class="math inline">\(2M\exp(-2\epsilon^2N)\)</span>。</p>
<p>实际上，经过一系列证明推导之后，可以该系数为： <span
class="math display">\[
2\cdot 2m_H(2N)\cdot\exp{(-2\cdot \frac{1}{16}\epsilon^2N)}
\]</span></p>
<blockquote>
<p>具体的数学证明相对比较复杂，本人也没有完全学习。这里仅记录林轩田老师在课程中提到的三个证明要点。</p>
</blockquote>
<p>第一点在于需要利用<span
class="math inline">\(E&#39;_{in}\)</span>来代替<span
class="math inline">\(E_{out}\)</span>。这里我们考虑<span
class="math inline">\(E&#39;_{in}\)</span>是一个在验证集verification set
<span class="math inline">\(D&#39;\)</span>上的误差，其中<span
class="math inline">\(|D&#39;| =
N\)</span>。也就是说利用验证集来评估模型的效果。如此，$|E_{in}-E_{out}|
<span class="math inline">\(的概率就可以转化成考虑\)</span>|E_{in} -
E'_{in}|$的概率，具体的转化结果如下： <span class="math display">\[
\frac{1}{2}P[\exists h \in H, s.t. |E_{in}(h) - E_{out}(h)| \ge \epsilon
]\\
\le
P[\exists h \in H, s.t. |E_{in}(h) - E&#39;_{in}(h)| \ge
\frac{\epsilon}{2} ]
\]</span>
第二点在于需要利用成长函数进行放缩。我们仍然是考虑坏事发生的可能性。对于假设空间来说，坏事发生的情况很多都是互相重叠的，最终有如下的结果：
<span class="math display">\[
\begin{aligned}
&amp;P[\exists h \in H, s.t. |E_{in}(h) - E_{out}(h)| \ge \epsilon ]
\\ \le&amp;
2 P[\exists h \in H, s.t. |E_{in}(h) - E&#39;_{in}(h)| \ge
\frac{\epsilon}{2}]
\\ \le&amp;
2 m_H(2N)P[\text{fixed}\ h \in H, s.t. |E_{in}(h) - E&#39;_{in}(h)| \ge
\frac{\epsilon}{2}]\\
\end{aligned}
\]</span>
第三点在于再次使用Hoeffding不等式进行替换，从而得到下面的结果： <span
class="math display">\[
\begin{aligned}
&amp;P[\exists h \in H, s.t. |E_{in}(h) - E_{out}(h)| \ge \epsilon ]
\\ \le&amp;
2 m_H(2N)P[\text{fixed}\ h \in H, s.t. |E_{in}(h) - E&#39;_{in}(h)| \ge
\frac{\epsilon}{2}]
\\ \le&amp;
2m_H(2N)\cdot 2\exp{(-2(\frac{\epsilon}{4})^2N)}
\end{aligned}
\]</span>
总而言之，整理之后，我们有对于无限容量的假设空间，我们可以得到下面的不等式成立：
<span class="math display">\[
P[\exists h \in H, s.t. |E_{in}(h) - E_{out}(h)| \ge \epsilon ]
\le 4m_H(2N)\exp{(-\frac{1}{8} \epsilon^2N )}
\]</span> 而在前面我们已经证明了，对于一个假设空间<span
class="math inline">\(H\)</span>来说，如果存在Break Point且值为<span
class="math inline">\(k\)</span>，则有： <span class="math display">\[
m_H(N) \le B(N,k) \le \sum_{i=0}^{k-1} \binom{N}{i}
\]</span> 这表示成长函数的上界是一个多项式，并且是<span
class="math inline">\(N-1\)</span>阶的，因此整体的系数变化主导是<span
class="math inline">\(N\)</span>。于是我们可以推导出，如果假设空间的Break
Point存在（用后面的话来说就是VC Dimension存在），当<span
class="math inline">\(N\)</span>足够大的时候，坏事发生的概率是很小的，从而有<span
class="math inline">\(E_{in} \approx
E_{out}\)</span>。又因为我们的假设空间无限大，学习算法完全可以从中学习到一个<span
class="math inline">\(E_{in}\)</span>相对较小的作为最终的输出映射，由此我们证明了在无限容量的假设空间上，机器学习的可行性。</p>
<h1 id="vc-bound-和-vc-dimension">VC Bound 和 VC Dimension</h1>
<p>在上面的推导中，我们得到了一个不等式： <span class="math display">\[
P[\exists h \in H, s.t. |E_{in}(h) - E_{out}(h)| \ge \epsilon ]
\le 4m_H(2N)\exp{(-\frac{1}{8} \epsilon^2N )}
\]</span> 这个不等式也就是著名的Vapink-Chervonenkis（VC）不等式。</p>
<p>同时，这里引入一个新的概念，称作<strong>VC
Dimension</strong>，记作<span
class="math inline">\(d_{vc}\)</span>。它和前面Break
Point的概念联系紧密。对于一个假设空间<span
class="math inline">\(H\)</span>来说，如果它的Break Point值为<span
class="math inline">\(k\)</span>，那么就有<span
class="math inline">\(d_{vc} = k-1\)</span>。<strong>Break
Point是第一个满足<span class="math inline">\(m_H(k) \neq
2^{k}\)</span>的值，VC Dimension则是最后一个满足<span
class="math inline">\(m_H(k) = 2^k\)</span>的值。</strong>VC
Dimension也是假设空间<span
class="math inline">\(H\)</span>的一个伴随性质，表示该假设空间最大完全正确的分类能力，在一定程度上也反映了假设空间的自由度。这里的<span
class="math inline">\(d_{vc}\)</span>仅与假设空间<span
class="math inline">\(H\)</span>有关，与学习算法<span
class="math inline">\(A\)</span>，输入分布<span
class="math inline">\(P\)</span>，目标映射<span
class="math inline">\(f\)</span>等都没有关系。</p>
<p>有了这个概念，我们可以继续上面不等式的整理： <span
class="math display">\[
\begin{aligned}
&amp;P[\exists h \in H, s.t. |E_{in}(h) - E_{out}(h)| \ge \epsilon ]
\\ \le&amp;
4m_H(2N)\exp{(-\frac{1}{8} \epsilon^2N )}
\\ \le &amp;
4B(2N,k)\exp{(-\frac{1}{8} \epsilon^2N )}
\\ \le &amp;
4\sum_{i=0}^{k-1}\binom{2N}{i}\exp{(-\frac{1}{8} \epsilon^2N )}
\\ \le &amp;
4(2N)^{k-1}\exp{(-\frac{1}{8} \epsilon^2N )}
\\ \le &amp;
4(2N)^{d_{vc}}\exp{(-\frac{1}{8} \epsilon^2N )}
\end{aligned}
\]</span> 通过VC不等式，我们可以大体估计需要的数据量，如果给定一个<span
class="math inline">\(\delta\)</span>&gt;0表示我们能够接受坏事的概率，以及精度<span
class="math inline">\(\epsilon &gt;0\)</span>，那么我们希望<span
class="math inline">\(4(2N)^{d_{vc}}\exp{(-\frac{1}{8} \epsilon^2N )}
\le\delta\)</span>，于是则有： <span class="math display">\[
N \ge \frac{8}{\epsilon^2}\ln{(\frac{4((2N)^{d_{vc}})}{\delta})}
\]</span>
由此就可以根据条件大体估计需要的数据量。但是实际上这个估计通常是一个过度估计，通常在小于这个数据量的时候，也能达到学习效果。这是因为我们在推导这个不等式的时候，利用了很多上界、放缩等技巧，导致这个边界实际上比较宽松。</p>
<h1 id="泛化边界">泛化边界</h1>
<p>结合VC Dimension的VC不等式如下： <span class="math display">\[
P[\exists h \in H, s.t. |E_{in}(h) - E_{out}(h)| \ge \epsilon ]
\le 4(2N)^{d_{vc}}\exp{(-\frac{1}{8} \epsilon^2N )}
\]</span> 在这个不等式中，我们利用$<span
class="math inline">\(来表示\)</span><span
class="math inline">\(，记\)</span>= 4(2N)^{d_{vc}}$，则可以计算得到：
<span class="math display">\[
\epsilon = \sqrt{\frac{8}{N}\ln{\frac{4(2N)^{d_{vc}}}{\delta}}}
\]</span> 重新描述不等式的含义，就是坏事情的概率不超过<span
class="math inline">\(\delta\)</span>，也就是好事情的概率最小为<span
class="math inline">\(1-\delta\)</span>。那么这里什么是好事情呢？就是<span
class="math inline">\(|E_{in}(h) - E_{out}(h)| \le
\epsilon\)</span>。所以有超过<span
class="math inline">\(1-\delta\)</span>的概率，有： <span
class="math display">\[
E_{in}(h) -\sqrt{\frac{8}{N}\ln{\frac{4(2N)^{d_{vc}}}{\delta}}} \le
E_{out}(h)
\le E_{in}(h) + \sqrt{\frac{8}{N}\ln{\frac{4(2N)^{d_{vc}}}{\delta}}}
\]</span> 也就是说我们已经推导出了泛化误差<span
class="math inline">\(E_{out}\)</span>的边界。不过一般来说，我们更加关心泛化误差的上界。记
<span class="math display">\[
\Omega(N,H,\delta)  =
\sqrt{\frac{8}{N}\ln{\frac{4(2N)^{d_{vc}}}{\delta}}}
\]</span> 则有泛化误差的上界为$E_{in} + (N,H,)
$，或者可以称其为泛化边界。</p>
<p>泛化误差上界告诉我们，我们至少有<span
class="math inline">\(1-\delta\)</span>的把握，可以说泛化误差$E_{out}E_{in}
+ (N,H,) <span class="math inline">\(。其中\)</span>(N,H,) <span
class="math inline">\(与样本数量\)</span>N<span
class="math inline">\(，假设空间\)</span>H<span
class="math inline">\(（\)</span>d_{vc}<span
class="math inline">\(）以及\)</span>$有关。这一项被称为模型复杂度，相关变量有如下的变化情况：</p>
<img src="/2023/08/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%AF%E8%A1%8C%E6%80%A7/%E8%AF%AF%E5%B7%AE%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%A4%8D%E6%9D%82%E5%BA%A6.png" srcset="/img/bilibiliTV.gif" lazyload class="" title="误差与模型复杂度">
<p>随着<span class="math inline">\(d_{vc}\)</span>的增大，训练误差<span
class="math inline">\(E_{in}\)</span>会减小，但是模型的复杂度<span
class="math inline">\(\Omega\)</span>会增加。这告诉我们模型并不是越复杂越好。模型复杂，虽然<span
class="math inline">\(E_{in}\)</span>降低了，但是与此同时模型复杂度<span
class="math inline">\(\Omega\)</span>会增加。因此好的模型需要综合考虑这两项，这也是正则化中增加惩罚项的缘由。</p>
<h1 id="总结与启示">总结与启示</h1>
<p>总结来说，本文主要说明了在有限容量和无限容量的假设空间上，机器学习的可行性。由此引出了VC
不等式以及VC Dimension的概念。这里的VC
Bound是一个比较宽松的边界，这是因为我们在推导过程中多次使用上界来进行替代。不过也正是因为这种宽松，VC
Bound的适用性很广，它能够适用于所有的机器学习，而与具体的模型、算法无关。</p>
<p>而抛开所有推导的细节，我们考虑结果给我们的启示意义。就是说，VC
Dimension基本上能够表征假设空间（模型）<span
class="math inline">\(H\)</span>的复杂程度，复杂程度越高，我们可以得到的训练误差也越低，但是同时也会导致<span
class="math inline">\(\Omega\)</span>项的增大。因此一个优秀的模型需要综合考虑这两部分，才能使得<span
class="math inline">\(E_{out}\)</span>足够小，使得模型具有良好的泛化能力。</p>
<h1 id="参考文章">参考文章</h1>
<ol type="1">
<li>《机器学习基石》- 林轩田 - Why Can Machine Learn</li>
<li>《机器学习》 - 周志华 - 第十二章 计算学习理论</li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/23949ca4f8ab">机器学习理论 - 简书
(jianshu.com)</a></li>
</ol>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="category-chain-item">机器学习</a>
  
  
    <span>></span>
    
  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA/" class="category-chain-item">基础理论</a>
  
  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="print-no-link">#机器学习</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>机器学习基础(2)-机器学习可行性</div>
      <div>https://evernorif.github.io/2023/08/18/机器学习基础-2-机器学习可行性/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>EverNorif</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年8月18日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/08/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-3-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" title="机器学习基础(3)-梯度下降">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">机器学习基础(3)-梯度下降</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/08/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/" title="机器学习基础(1)-机器学习概论">
                        <span class="hidden-mobile">机器学习基础(1)-机器学习概论</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <i class="iconfont icon-love"></i> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  var relativeDate = function() {
    var updatedTime = document.getElementById('updated-time');
    if (updatedTime) {
      var text = updatedTime.textContent;
      var reg = /\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:Z|[+-]\d{2}:\d{2})/;
      var matchs = text.match(reg);
      if (matchs) {
        var relativeTime = moment(matchs[0]).fromNow();
        updatedTime.textContent = text.replace(reg, relativeTime);
      }
      updatedTime.style.display = '';
    }
  };
  Fluid.utils.createScript('https://lib.baomitu.com/moment.js/2.29.4/moment.min.js', function() {
    if (!'zh-cn'.startsWith('en')) {
      Fluid.utils.createScript('https://lib.baomitu.com/moment.js/2.29.4/locale/zh-cn.min.js', function() {
        relativeDate();
      });
    } else {
      relativeDate();
    }
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.0/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.1/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/xiaomai.model.json"},"display":{"position":"left","width":150,"height":300,"vOffset":-90},"mobile":{"show":true},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
