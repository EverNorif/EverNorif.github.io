

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/myfavicon.png">
  <link rel="icon" href="/img/myfavicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="EverNorif">
  <meta name="keywords" content="">
  
    <meta name="description" content="本篇主要介绍了决策树模型。决策树通过构造一个树形结构来逐步决策，直到最终的决策结果。决策树的学习算法包括了ID3、C4.5和CART。">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习基础(8)-决策树">
<meta property="og:url" content="http://example.com/2023/08/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-8-%E5%86%B3%E7%AD%96%E6%A0%91/index.html">
<meta property="og:site_name" content="EverNorif">
<meta property="og:description" content="本篇主要介绍了决策树模型。决策树通过构造一个树形结构来逐步决策，直到最终的决策结果。决策树的学习算法包括了ID3、C4.5和CART。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2023/08/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-8-%E5%86%B3%E7%AD%96%E6%A0%91/mes.png">
<meta property="article:published_time" content="2023-08-23T13:04:09.000Z">
<meta property="article:modified_time" content="2023-08-23T13:08:13.548Z">
<meta property="article:author" content="EverNorif">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="监督学习">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/2023/08/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-8-%E5%86%B3%E7%AD%96%E6%A0%91/mes.png">
  
  
  
  <title>机器学习基础(8)-决策树 - EverNorif</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.4","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/bilibiliTV.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 80vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>EverNorif</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tools/">
                <i class="iconfont icon-briefcase"></i>
                <span>工具</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button"
                 data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                <i class="iconfont icon-bilibili"></i>
                <span>番剧</span>
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                
                  
                  
                  
                  <a class="dropdown-item" href="/bangumis/">
                    <i class="iconfont icon-bilibili-fill"></i>
                    <span>追番</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/cinemas/">
                    <i class="iconfont icon-youtube-fill"></i>
                    <span>追剧</span>
                  </a>
                
              </div>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/post.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="机器学习基础(8)-决策树"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-08-23 21:04" pubdate>
          2023年8月23日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          8.3k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          <!-- compatible with older versions-->
          69 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
        <div class="scroll-down-bar">
          <i class="iconfont icon-arrowdown"></i>
        </div>
      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">机器学习基础(8)-决策树</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="决策树模型">决策树模型</h1>
<p>决策树是一种基本的分类与回归问题，它模拟人类进行决策的过程，经过一系列的if-then过程达到最后的决策结果。决策树整体是一个树形结构，结点则分为两种类型：内部结点以及叶子结点。内部结点表示根据某个属性（特征）进行划分，决定进入哪一个子结点中；而叶子结点则是最终的输出结果，在分类问题中就是一个类别，在回归问题中就是一个数值。决策树的决策过程就是从根结点到叶子结点的一条路径，路径中的内部结点决定下一步的走向，叶子结点决定最终的输出。</p>
<p>决策树学习本质上是从训练数据集中归纳出一组分类规则。与训练数据集不相矛盾的决策树可能有多个，也可能一个都没有。我们希望得到的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。同其他机器学习算法类似，我们也希望用损失函数来表征决策树的效果。确定损失函数之后，决策树学习就变为了在损失函数意义下选择最优决策树的问题。</p>
<p>但是，在所有可能的决策树中选取最优决策树是一个NP完全问题，所以在<strong>现实中决策树学习算法通常是采用启发式学习，近似求解这一最优化问题</strong>。简单来说，我们使用贪婪的思想，<strong>在每次进行内部结点划分的时候，都采用最优的划分方式，递归地选择最优特征</strong>。这样得到的决策树是次最优的。</p>
<p>按照递归的方式进行决策树的构建，我们很容易得到一个完全长成的树。可以预见的是，这棵树可能比较复杂，它在训练集上有很好的分类能力，很容易达到训练误差<span
class="math inline">\(E_{in}=0\)</span>（极端情况下，完全长成的树上每个叶子结点都只有一个训练样本，相当于对每个训练样本都特殊处理）。这样的树对未知的数据通常有较差的效果，即发生了过拟合现象。因此我们需要对已生成的树自下而上地进行剪枝，降低树的复杂度，从而使它具有更好的泛化能力。</p>
<p>总结来说，决策树学习算法包含<strong>特征选择</strong>、<strong>决策树的生成</strong>以及<strong>决策树的剪枝</strong>。决策树的生成对应模型的局部选择，考虑局部最优；而决策树的剪枝对应于模型的全局选择，考虑全局最优。</p>
<p>决策树的优点是模型直观，便于理解并且应用广泛，同时它算法简单，容易实现，在训练和预测的时候有较高的效率。不过决策树也有相应的缺点，它缺少足够的理论支持，虽然能够达到好的效果，但是如何选择合适的树结构也是困难所在。</p>
<h1 id="决策树学习">决策树学习</h1>
<p>决策树的学习通常包括三个步骤：特征选择、决策树的生成以及决策树的修剪（剪枝）。</p>
<h2 id="特征选择">特征选择</h2>
<p>特征选择是在每一个内部结点中进行的过程。考虑在目前的内部结点上，训练数据集为<span
class="math inline">\(D =
\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}\)</span>，其中<span
class="math inline">\(x_i\)</span>为一个多维向量<span
class="math inline">\(x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(m)})\)</span>，每个维度代表一个特征取值；<span
class="math inline">\(y_i\)</span>表示对应的类别取值，代表一个类别。内部结点需要做的事情就是选择一个特征<span
class="math inline">\(A\)</span>，根据特征<span
class="math inline">\(A\)</span>的取值可以将训练数据集划分为n个子集<span
class="math inline">\(D_1,D_2,...,D_n\)</span>，对应生成的<span
class="math inline">\(n\)</span>个子结点。选择的依据则有信息增益、信息增益比以及基尼系数。</p>
<h3 id="信息增益">信息增益</h3>
<p>首先需要给出熵和条件熵的概念。在信息论中，熵（entropy）用来度量随机变量的不确定性。假设<span
class="math inline">\(X\)</span>是一个取有限个值的离散随机变量，它的概率分布如下：
<span class="math display">\[
P(X = x_i) = p_i,\quad i=1,2,...,n
\]</span> 则随机变量<span class="math inline">\(X\)</span>的熵定义为：
<span class="math display">\[
H(X) = -\sum_{i=1}^np_i\log p_i
\]</span> 其中如果<span
class="math inline">\(p_i=0\)</span>，则定义<span
class="math inline">\(0\log 0 =
0\)</span>。其中的对数项，底数通常取2或者自然对数<span
class="math inline">\(e\)</span>，此时熵的单位分别为比特（bit）或纳特（nat）。从定义中我们可以看到，熵只依赖于<span
class="math inline">\(X\)</span>的分布，而不依赖于具体的随机变量取值，因此也可以将<span
class="math inline">\(X\)</span>的熵记作<span class="math inline">\(H(p)
= H(X)\)</span>。当熵<span class="math inline">\(H(p)
=0\)</span>时，则表示随机变量完全没有不确定性。</p>
<p>条件熵则涉及到多个随机变量。设有随机变量<span
class="math inline">\((X,Y)\)</span>，它的联合概率分布为： <span
class="math display">\[
P(X=x_i,Y=y_i) = p_{ij},\quad i=1,2,...,n,\quad j=1,2,...,m
\]</span> 条件熵<span
class="math inline">\(H(Y|X)\)</span>表示在已知随机变量<span
class="math inline">\(X\)</span>的条件下，随机变量<span
class="math inline">\(Y\)</span>的不确定性，定义为在<span
class="math inline">\(X\)</span>给定的条件下，<span
class="math inline">\(Y\)</span>的条件概率分布的熵对<span
class="math inline">\(X\)</span>的数学期望： <span
class="math display">\[
H(Y|X) = \sum_{i=1}^np_iH(Y|X= x_i)
\]</span> 其中，<span class="math inline">\(p_i =
P(X=x_i),i=1,2,...,n\)</span>。</p>
<p>当熵和条件熵中的概率是由数据估计得到的，所对应的熵和条件熵通常也分别被称为经验熵和经验条件熵。</p>
<p>信息增益（information
gain）的概念需要借助熵以及条件熵，它表示的是在得知特征<span
class="math inline">\(X\)</span>的信息之后，使得类<span
class="math inline">\(Y\)</span>信息的不确定性减少的程度。信息增益是针对某个特征<span
class="math inline">\(A\)</span>以及某个训练数据集<span
class="math inline">\(D\)</span>来说的，信息增益<span
class="math inline">\(g(D,A)\)</span>定义为集合<span
class="math inline">\(D\)</span>的熵<span
class="math inline">\(H(D)\)</span>与在特征<span
class="math inline">\(A\)</span>给定条件下<span
class="math inline">\(D\)</span>的条件熵<span
class="math inline">\(H(D|A)\)</span>之差，即： <span
class="math display">\[
g(D,A) = H(D) - H(D|A)
\]</span></p>
<blockquote>
<p>一般地，熵<span class="math inline">\(H(Y)\)</span>与条件熵<span
class="math inline">\(H(Y|X)\)</span>之差称为互信息（mutual
information）。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。</p>
</blockquote>
<p>信息增益依赖于某个特征<span class="math inline">\(A\)</span>。熵<span
class="math inline">\(H(D)\)</span>表示直接对训练数据集<span
class="math inline">\(D\)</span>进行分类的不确定性，而条件熵<span
class="math inline">\(H(D|A)\)</span>表示在特征<span
class="math inline">\(A\)</span>给定的条件下对数据集<span
class="math inline">\(D\)</span>分类的不确定性。显然在给定某个特征后，不确定性应该有所下降，否则就是这个特征对于分类基本没有用。而不同的特征往往具有不同的信息增益，信息增益大的特征具有更强的分类能力，我们在内部结点进行特征选择的时候，也是选择信息增益最大的特征进行划分。</p>
<p>总结来说，根据信息增益进行划分的时候，我们会依次计算每个特征的信息增益。假设此时选择了特征<span
class="math inline">\(A\)</span>。对于数据集<span
class="math inline">\(D\)</span>，基于它的分类结果可以得到一个随机变量，从而得到熵<span
class="math inline">\(H(D)\)</span>。而根据<span
class="math inline">\(A\)</span>的取值，我们可以将数据集<span
class="math inline">\(D\)</span>划分为n个子集<span
class="math inline">\(D_1,D_2,...,D_n\)</span>。对于每个子集<span
class="math inline">\(D_i\)</span>，基于分类结果，我们可以得到熵<span
class="math inline">\(H(D_i)\)</span>，而条件熵<span
class="math inline">\(H(D|A)\)</span>就是熵<span
class="math inline">\(H(D_i)\)</span>的期望，<span
class="math inline">\(D_i\)</span>对应的概率就是<span
class="math inline">\(|D_i|/|D|\)</span>。</p>
<h3 id="信息增益比">信息增益比</h3>
<p>以信息增益作为依据进行特征选择，可能会出现某种偏向，偏向于选择取值较多的特征。而信息增益比（information
gain ratio）可以对这一问题进行校正。信息增益比定义如下： <span
class="math display">\[
g_R(D,A) = \frac{g(D,A)}{H_A(D)} = \frac{H(D)-H(D|A)}{H_A(D)}
\]</span> 其中，<span
class="math inline">\(H_A(D)\)</span>表示数据集<span
class="math inline">\(D\)</span>基于特征<span
class="math inline">\(A\)</span>取值的熵，<span
class="math inline">\(n\)</span>表示特征<span
class="math inline">\(A\)</span>有<span
class="math inline">\(n\)</span>个取值： <span class="math display">\[
H_A(D) = -\sum_{i=1}^{n} \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|}
\]</span>
在特征选择时，会依次计算所有特征的信息增益比，然后选择最大的那个特征作为划分依据。</p>
<h3 id="基尼系数">基尼系数</h3>
<p>同样假设<span
class="math inline">\(X\)</span>是一个取有限个值的离散变量，有如下的概率分布：
<span class="math display">\[
P(X = x_i) = p_i,\quad i=1,2,...,n
\]</span> 则随机变量<span
class="math inline">\(X\)</span>的基尼系数定义为： <span
class="math display">\[
Gini(X) = \sum_{i=1}^n p_i(1-p_i) = 1 - \sum_{i=1}^n p_i^2
\]</span> 对于训练数据集<span
class="math inline">\(D\)</span>，针对特征<span
class="math inline">\(A\)</span>可以将其划分为若干个子集<span
class="math inline">\(D_1,D_2,...,D_n\)</span>。对于每个子集<span
class="math inline">\(D_i\)</span>，我们可以计算出它基于分类结果的基尼系数<span
class="math inline">\(Gini(D_i)\)</span>，而整体基于特征<span
class="math inline">\(A\)</span>的基尼系数则是子集基尼系数<span
class="math inline">\(Gini(D_i)\)</span>的期望，<span
class="math inline">\(D_i\)</span>对应的概率就是<span
class="math inline">\(|D_i|/|D|\)</span>。 <span class="math display">\[
Gini(D,A) = \sum_{i=1}^n\frac{|D_i|}{|D|}Gini(D_i)
\]</span> 基尼系数<span
class="math inline">\(Gini(D)\)</span>表示集合<span
class="math inline">\(D\)</span>的不确定性，而<span
class="math inline">\(Gini(D,A)\)</span>表示经过<span
class="math inline">\(A\)</span>划分之后集合<span
class="math inline">\(D\)</span>的不确定性。基尼系数越大，集合的不确定性也就越大，这点与熵是相似的。基于基尼系数进行特征选择时，我们会选择对应基尼系数最小的那个特征，它对应得到的集合不确定性也最小。</p>
<h2 id="决策树剪枝">决策树剪枝</h2>
<p>有了特征选择的依据，我们就可以递归地进行决策树生成。完成生成的树通常会发生过拟合的现象，因此我们需要考虑决策树的复杂度，进行决策树剪枝，对已生成的决策树进行简化。剪枝（pruning）从已经生成的树上裁掉一些子树或者叶子结点，并将其根结点或者父结点作为新的叶子结点，从而简化树模型。</p>
<p>极小化决策树整体的损失函数是一种剪枝的方式。假设生成的决策树为<span
class="math inline">\(T\)</span>，它的叶子结点的个数为<span
class="math inline">\(|T|\)</span>，并且<span
class="math inline">\(t\)</span>是树<span
class="math inline">\(T\)</span>的叶子结点，该叶子结点中有<span
class="math inline">\(N_t\)</span>个样本点，则决策树学习的损失函数可以定义如下：
<span class="math display">\[
\begin{aligned}
C_{\alpha}(T) &amp;= C(T) + \alpha |T| \\
&amp;= \sum_{t=1}^{|T|}N_t H_t(T) + \alpha|T|
\end{aligned}
\]</span> 其中<span
class="math inline">\(H_t(T)\)</span>表示在叶子结点<span
class="math inline">\(t\)</span>上基于分类结果得到的经验熵；<span
class="math inline">\(C(T)\)</span>表示模型对训练数据的预测误差，它是所有叶子结点上经验熵的加权<span
class="math inline">\(N_t\)</span>和；<span
class="math inline">\(|T|\)</span>是叶子结点的个数，一定程度上表示模型的复杂度；<span
class="math inline">\(\alpha \ge
0\)</span>控制训练误差与模型复杂度之间的影响。</p>
<p>剪枝就是在<span
class="math inline">\(\alpha\)</span>确定的情况下，选择损失函数最小的模型。实际上，这里定义的损失函数极小化等价于正则化的极大似然估计。</p>
<p>剪枝算法整体是一个自下而上的过程。对一棵完全生成的树<span
class="math inline">\(T\)</span>以及给定的参数<span
class="math inline">\(\alpha\)</span>，我们首先计算每个结点的经验熵，之后递归地从树的叶子结点向上回缩。当然回缩是有条件的，假设叶子结点回缩前后整体树分别为<span
class="math inline">\(T_B\)</span>和<span
class="math inline">\(T_A\)</span>，则当<span
class="math inline">\(C_{\alpha}(T_A) \le
C_{\alpha}(T_B)\)</span>时，才进行叶结点回缩。递归进行直到无法回缩，我们就可以得到损失函数最小的树模型。</p>
<h2 id="学习算法">学习算法</h2>
<p>常见的决策树学习算法包括ID3、C4.5以及CART。</p>
<h3 id="id3">ID3</h3>
<p>ID3算法的核心是在决策树的各个结点上应用信息增益准则进行特征选择，递归地构建决策树。从根结点开始进行特征选择，递归进行决策树构建，直到所有特征的信息增益均很小或者没有特征可以选择为止。我们一般会给定一个阈值$$，如果当前结点上经过选择后特征对应的信息增益都小于这个阈值，则不再进行递归划分，而它将成为一个单结点树，返回的结果则是对应实例中出现次数最多的类别。</p>
<p>ID3算法中只有树的生成，所以该算法生成的树容易产生过拟合。</p>
<h3 id="c4.5">C4.5</h3>
<p>C4.5算法与ID3算法相似，差别在于C4.5在特征选择的时候使用了信息增益比（也存在阈值$$的使用）。</p>
<h3 id="cart">CART</h3>
<p>CART全称为分类与回归树（Classification And Regression
Tree），是一种应用广泛的决策树学习方法。CART同样包括特征选择、决策树生成以及决策树剪枝。它既可以用于分类也可以用于回归。</p>
<p>CART中假设决策树都是二叉树，即每次特征选择后将数据集划分成两份，递归地二分每个特征。</p>
<ul>
<li>CART决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大</li>
<li>CART决策树剪枝：用验证数据集对已生成的树进行剪枝，并选择最优子树，此时用损失函数最小作为剪枝的标准</li>
</ul>
<h4 id="cart生成">CART生成</h4>
<p>CART决策树的生成指的是递归地构建二叉决策树的过程。其中又可以分为回归树和分类树，对回归树使用平方误差最小化准则，对分类树用基尼系数最小化准则。</p>
<p>一棵回归树对应输入空间的一个划分以及在划分单元上的输出值。类比于分类树，回归树的叶子结点上输出的是一个固定的值，而分类树则是类别标签。当输入空间的划分确定之后，可以使用平方误差来表示回归树对训练数据的预测误差，同时通过最小化预测误差来决定当前结点输出什么样的值。因此对于CART回归树来说，接下来需要确定的就是如何进行空间划分。</p>
<p>对于CART回归树来说，内部结点得到的训练资料是<span
class="math inline">\(D=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}\)</span>，其中<span
class="math inline">\(y_i\)</span>是连续变量，<span
class="math inline">\(x_i\)</span>是一个多维向量<span
class="math inline">\((x_i^{(1)},x_i^{(2)},...,x_i^{(n)})\)</span>。CART每次的选择都是进行二分。对于第<span
class="math inline">\(i\)</span>个输入的第<span
class="math inline">\(j\)</span>个维度分量<span
class="math inline">\(x_i^{(j)}\)</span>，假设取值为<span
class="math inline">\(x_i^{(j)} =
s\)</span>，那么根据这个值，就可以将输入空间划分为两个部分： <span
class="math display">\[
R_1(j,s) = \{x|x^{(j)} \le s\},\quad R_2(j,s) = \{x|x^{(j)} &gt; s\}
\]</span> 对于固定的<span
class="math inline">\(i\)</span>，我们可以找到最优切分点<span
class="math inline">\(s\)</span>，就是说对于给定的输入<span
class="math inline">\(x_i\)</span>，确定以它的第几个维度分量进行划分能够做到预测误差最小化，以即完成最优化：
<span class="math display">\[
\min_{j,s} [\min_{c_1} \sum_{x_i \in R_1(j,s)} (y_i - c_1)^2 +
\min_{c_2} \sum_{x_i \in R_2(j,s)} (y_i - c_2)^2]
\]</span> 也就是对于给定的输入变量<span
class="math inline">\(x_i\)</span>，我们能够得到一个最优的划分<span
class="math inline">\((j,s)\)</span>。之后遍历所有的输入变量，找到全局的最优划分，以最优化分作为当前内部结点的划分。递归进行上述的划分操作，这样就生成一棵回归树。这样的回归树通常被称为最小二乘回归树。</p>
<p>CART分类树，内部结点的特征选择则是基于基尼系数的。不过需要注意的是，由于CART都是二分决策的，因此对于选定的特征<span
class="math inline">\(A\)</span>，每次按照特征<span
class="math inline">\(A\)</span>的某个取值<span
class="math inline">\(a\)</span>进行划分，得到两个子集<span
class="math inline">\(D_1,D_2\)</span>，基尼系数的计算如下，每次选择能够使得基尼系数最小的特征以及取值。
<span class="math display">\[
Gini(D,A) = \frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|} Gini(D_2)
\]</span> 算法停止计算的条件有三种：</p>
<ol type="1">
<li>结点中的样本个数小于预定阈值</li>
<li>结点中最优基尼系数小于预定阈值</li>
<li>没有更多的特征或类别</li>
</ol>
<h4 id="cart剪枝">CART剪枝</h4>
<p>CART剪枝算法通过从完全长成的决策树底端剪去一些子树，降低模型的复杂度。具体来说，CART剪枝算法由两步组成：</p>
<ol type="1">
<li>从完全生成的决策树<span
class="math inline">\(T_0\)</span>的底端开始不断剪枝直到<span
class="math inline">\(T_0\)</span>的根结点，形成一个子树序列<span
class="math inline">\(\{T_0,T_1,...,T_n\}\)</span></li>
<li>利用交叉验证的方法在独立的验证数据集上对子树序列进行测试，从中选择最优的子树</li>
</ol>
<p>在生成子树序列的过程中，我们需要使用到树的损失函数： <span
class="math display">\[
C_{\alpha}(T) = C(T) + \alpha|T|
\]</span> 对于固定的<span
class="math inline">\(\alpha\)</span>，我们一定能够找到使得损失函数<span
class="math inline">\(C_{\alpha}(T)\)</span>最小的子树，将其记为<span
class="math inline">\(T_{\alpha}\)</span>。当<span
class="math inline">\(\alpha\)</span>偏大的时候，最优子树偏小；反之最优子树偏大。</p>
<p>生成子树序列的过程同样伴随着<span
class="math inline">\(\alpha\)</span>的增大。具体来说，我们将<span
class="math inline">\(\alpha\)</span>从小增大，得到<span
class="math inline">\(0=\alpha_0 &lt; \alpha_1&lt;...&lt;\alpha_n
&lt;+\infty\)</span>，如此产生了一系列区间<span
class="math inline">\([\alpha_i,\alpha_{i+1}),i=0,1,...,n\)</span>。我们剪枝得到的子树序列恰好对应于这些区间，更具体地说，子树<span
class="math inline">\(T_i\)</span>对应区间<span
class="math inline">\([\alpha_i,\alpha_{i+1})\)</span>，它表示在<span
class="math inline">\(\alpha \in
[\alpha_i,\alpha_{i+1})\)</span>的情况下，我们能够得到的最优子树。</p>
<p>假设我们从<span class="math inline">\(T_0\)</span>开始剪枝，对于<span
class="math inline">\(T_0\)</span>的任意内部结点<span
class="math inline">\(t\)</span>，以<span
class="math inline">\(t\)</span>为单结点树，损失函数为： <span
class="math display">\[
C_{\alpha}(t) = C(t) + \alpha
\]</span> 而以<span
class="math inline">\(t\)</span>为根结点的子树，损失函数为： <span
class="math display">\[
C_{\alpha}(T_t) = C(T_t) + \alpha|T_t|
\]</span> 当<span
class="math inline">\(\alpha=0\)</span>以及充分小的时候，有： <span
class="math display">\[
C_{\alpha}(T_t) &lt; C_{\alpha}(t)
\]</span> 随着<span
class="math inline">\(\alpha\)</span>的增大，会存在某个<span
class="math inline">\(\alpha\)</span>，有： <span
class="math display">\[
C_{\alpha}(T_t) = C_{\alpha}(t)
\]</span> 此时有： <span class="math display">\[
\alpha = \frac{C(t) - C(T_t)}{|T_t| - 1}
\]</span> 于是，我们需要做的就是对<span
class="math inline">\(T_0\)</span>的每个内部结点<span
class="math inline">\(t\)</span>，计算： <span class="math display">\[
g(t) = \frac{C(t) - C(T_t)}{|T_t| - 1}
\]</span> 这表示剪枝后整体损失函数减少的程度。在<span
class="math inline">\(T_0\)</span>中减去<span
class="math inline">\(g(t)\)</span>最小的子树<span
class="math inline">\(T_{t}\)</span>，将得到的树作为<span
class="math inline">\(T_1\)</span>，同时更新<span
class="math inline">\(\alpha\)</span>，将最小的<span
class="math inline">\(g(t)\)</span>设置为<span
class="math inline">\(\alpha_1\)</span>。重复这个过程，不断增加<span
class="math inline">\(\alpha\)</span>的值，得到一系列区间，得到一系列子树，直到得到根结点。如此就得到了子树序列。整个过程相当于每次对树进行一次修剪，得到子树构成子树序列，然后通过交叉验证选择表现最好的子树作为最终的输出。</p>
<h1 id="附加参考">附加参考</h1>
<h2 id="信息熵">信息熵</h2>
<p>首先我们需要引入信息量的概念。所谓信息量，是给我们带来确定性的。如果一件事情由不确定变成了确定的，那它就给我们带来了信息量。并且我们可以说，信息量衡量的是从不确定变成确定的难度。而既然说到了不确定性，当然应该和概率有关系。在这里，我们假设函数<span
class="math inline">\(f\)</span>来表示信息量，并且在本节的下面篇幅中，<span
class="math inline">\(f\)</span>均代表信息量的计算： <span
class="math display">\[
f(x):= \text{信息量}
\]</span> 考虑下面的图：</p>
<img src="/2023/08/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-8-%E5%86%B3%E7%AD%96%E6%A0%91/mes.png" srcset="/img/bilibiliTV.gif" lazyload class="">
<p>从事件A到事件C，有<span
class="math inline">\(p_3\)</span>的概率；从事件A到事件B，有<span
class="math inline">\(p_1\)</span>的概率；从事件B到事件C，有<span
class="math inline">\(p_2\)</span>的概率。</p>
<p>如果最终从事件A到了事件C，那么无论是从什么路径到达的，它带来的信息量应该是一样多的，并且结合概率公式，应该有如下条件：
<span class="math display">\[
f(p_3) = f(p_1)+f(p_2)\\
p_3 = p_1 \times p_2
\]</span> 因为<span
class="math inline">\(f\)</span>需要满足上面的式子，我们可以定义它使用log：
<span class="math display">\[
f(x):=?\log_?x
\]</span>
这样还需要确定系数和底数。对于信息量来说，这里的x表示的是事件发生的概率。越小概率的事件，如果发生了对应的信息量应该越大，函数<span
class="math inline">\(f\)</span>应该是一个递减函数，于是我们将系数定义为-1。此时我们可以做到自洽，底数的选择并不会影响后续逻辑。（这里选择底数为2）</p>
<p>这样信息量的表达式最终就写成： <span class="math display">\[
f(x):=-\log_2x
\]</span>
接下来考虑的就是熵的定义。信息量考虑的是一个事件，而熵考虑的是一个系统内的所有事件（式子中的<span
class="math inline">\(P\)</span>​指的是在一个概率系统当中）。 <span
class="math display">\[
H(P):=\text{熵}
\]</span> 假设一个系统内可能发生事件<span
class="math inline">\(X,Y,Z\)</span>​，他们发生的概率分别为<span
class="math inline">\(p_1,p_2,p_3\)</span>​。由上面我们知道，他们对应的信息量为<span
class="math inline">\(f(p_1),f(p_2),f(p_3)\)</span>。但是信息量指的是一个事件发生之后带来的，如果事件没有发生，就没有这么多信息量。所以系统的熵不应该是各事件信息量的简单加和，而应该结合对应事件的概率进行加权：
<span class="math display">\[
\begin{aligned}
H(P=\{ X:p_1,Y:p_2,Z:p_3\}) &amp;:=p_1f(p_1)+p_2f(p_2)+p_3f(p_3)\\
&amp;=p_1(-\log_2p_1)+p_3(-\log_2p_2)+p_3(-\log_2p_3)
\end{aligned}
\]</span> 于是，对于一个概率系统<span
class="math inline">\(P\)</span>来说，熵即为对概率系统中的信息量求期望：
<span class="math display">\[
\begin{aligned}
H(P) &amp;:= E(P_f)\\
&amp;=\sum_i p_if(p_i)
\\
&amp;=\sum_ip_i(-\log_2p_i)
\\
&amp;=-\sum_ip_i \cdot \log_2p_i
\end{aligned}
\]</span></p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="category-chain-item">机器学习</a>
  
  
    <span>></span>
    
  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA/" class="category-chain-item">基础理论</a>
  
  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">#机器学习</a>
      
        <a href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/">#监督学习</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>机器学习基础(8)-决策树</div>
      <div>http://example.com/2023/08/23/机器学习基础-8-决策树/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>EverNorif</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年8月23日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/08/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-7-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM%E4%B8%8E%E6%A0%B8%E6%8A%80%E5%B7%A7/" title="机器学习基础(7)-支持向量机SVM与核技巧">
                        <span class="hidden-mobile">机器学习基础(7)-支持向量机SVM与核技巧</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <i class="iconfont icon-love"></i> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.0/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.1/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/xiaomai.model.json"},"display":{"position":"left","width":150,"height":300,"vOffset":-90},"mobile":{"show":true},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
