

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/myfavicon.png">
  <link rel="icon" href="/img/myfavicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="EverNorif">
  <meta name="keywords" content="">
  
    <meta name="description" content="SparkSQL是Spark中提供用来操作结构性数据的一个模块，这里简单介绍一下SparkSQL，首先对SparkSQL有一个初步印象，然后对如何利用SparkSQL进行编程做了简单讲解，包括DataFrame和DataSet的简单使用、如何在SparkSQL中使用SQL、用户自定义函数的书写以及数据加载与保存。">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark学习笔记-SparkSQL(1)-概述以及编程入门">
<meta property="og:url" content="http://example.com/2022/07/26/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-SparkSQL-1-%E6%A6%82%E8%BF%B0%E4%BB%A5%E5%8F%8A%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8/index.html">
<meta property="og:site_name" content="EverNorif">
<meta property="og:description" content="SparkSQL是Spark中提供用来操作结构性数据的一个模块，这里简单介绍一下SparkSQL，首先对SparkSQL有一个初步印象，然后对如何利用SparkSQL进行编程做了简单讲解，包括DataFrame和DataSet的简单使用、如何在SparkSQL中使用SQL、用户自定义函数的书写以及数据加载与保存。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2022/07/26/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-SparkSQL-1-%E6%A6%82%E8%BF%B0%E4%BB%A5%E5%8F%8A%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8/%E5%88%9D%E5%A7%8B%E5%AF%B9%E8%B1%A1.png">
<meta property="article:published_time" content="2022-07-26T00:15:02.000Z">
<meta property="article:modified_time" content="2022-07-26T04:43:50.000Z">
<meta property="article:author" content="EverNorif">
<meta property="article:tag" content="笔记">
<meta property="article:tag" content="Spark">
<meta property="article:tag" content="SparkSQL">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/2022/07/26/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-SparkSQL-1-%E6%A6%82%E8%BF%B0%E4%BB%A5%E5%8F%8A%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8/%E5%88%9D%E5%A7%8B%E5%AF%B9%E8%B1%A1.png">
  
  
  
  <title>Spark学习笔记-SparkSQL(1)-概述以及编程入门 - EverNorif</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.0","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/bilibiliTV.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>

  
<meta name="generator" content="Hexo 6.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 80vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>EverNorif</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tools/">
                <i class="iconfont icon-briefcase"></i>
                工具
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button"
                 data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                <i class="iconfont icon-bilibili"></i>
                番剧
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                
                  
                  
                  
                  <a class="dropdown-item" href="/bangumis/">
                    <i class="iconfont icon-bilibili-fill"></i>
                    追番
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/cinemas/">
                    <i class="iconfont icon-youtube-fill"></i>
                    追剧
                  </a>
                
              </div>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/post.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Spark学习笔记-SparkSQL(1)-概述以及编程入门"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-07-26 08:15" pubdate>
          2022年7月26日 早上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          13k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          <!-- compatible with older versions-->
          111 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
        <div class="scroll-down-bar">
          <i class="iconfont icon-arrowdown"></i>
        </div>
      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Spark学习笔记-SparkSQL(1)-概述以及编程入门</h1>
            
            <div class="markdown-body">
              
              <h1 id="SparkSQL概述"><a href="#SparkSQL概述" class="headerlink" title="SparkSQL概述"></a>SparkSQL概述</h1><p>SparkSQL是Spark用于结构化数据处理的一个模块。SparkSQL可以简化RDD的开发，提高开发效率，并且执行效率高。我们可以在SparkSQL中使用SQL语句来完成查询，同时也可以使用SparkSQL提供的接口来完成开发。</p>
<p>Spark具有如下特点：</p>
<ol>
<li>易整合：无缝地整合了SQL查询和Spark编程，我们可以很容易在Spark编程中书写SQL语句</li>
<li>统一的数据访问：我们可以使用相同的方式连接不同的数据源</li>
<li>兼容Hive：可以在已有的数据仓库上直接运行SQL或者HiveSQL</li>
<li>标准数据连接：允许通过JDBC或者ODBC来进行连接</li>
</ol>
<p>SparkSQL中为我们提供了两个编程抽象，DataFrame和DataSet</p>
<p>DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库中的二维表。DataFrame与RDD的主要区别在于，RDD中仅包括数据，而DataFrame中还包括数据schema的信息，即每一列的名称和类型等，即DataFrame携带更多的结构信息，我们可以将它当作数据库中的一张表来对待。</p>
<p>DataSet是在Spark1.6中添加的一个新抽象，是DataFrame的一个扩展。在RDD中我们关注数据的类型，在DataFrame中我们关注每一列的数据和类型，DataSet则融合了两者的优点，即既有每行数据的类型，又有每列数据的类型。</p>
<ul>
<li>使用样例类来定义DataSet中数据的结构信息，样例类中每个属性的名称直接映射到DataSet中的字段名称</li>
<li>DataSet是强类型的，例如<code>DataSet[Car], DataSet[Person]</code>等，可以理解为表中的每一行都是一个数据类型</li>
<li>DataFrame是DataSet的一个特例，每一行的数据类型为<code>Row</code>，即<code>DataFrame=DataSet[Row]</code>。Row是一个类型，获取数据的时候需要指定顺序。</li>
</ul>
<h1 id="简单演示"><a href="#简单演示" class="headerlink" title="简单演示"></a>简单演示</h1><p>我们可以在Spark的交互式命令行窗口中简单演示SparkSQL的使用。在前面我们执行Spark应用程序，需要首先构建上下文对象SparkContext。SparkSQL可以理解为对Spark Core的一种封装，不仅在模型上进行了封装，而且对上下文环境对象也进行了封装。我们在查询前，需要构建查询起点<code>SparkSession</code>，在命令行中，已经存在有该参数，名称为spark。</p>
<img src="/2022/07/26/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-SparkSQL-1-%E6%A6%82%E8%BF%B0%E4%BB%A5%E5%8F%8A%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8/%E5%88%9D%E5%A7%8B%E5%AF%B9%E8%B1%A1.png" srcset="/img/bilibiliTV.gif" lazyload class="" title="初始对象">

<h2 id="创建DataFrame"><a href="#创建DataFrame" class="headerlink" title="创建DataFrame"></a>创建DataFrame</h2><p>创建DataFrame有三种方式：通过Spark的数据源进行创建；从一个存在的RDD进行创建；从Hive进行查询返回。这里我们使用Spark从数据源进行创建，数据源是一个json文件，其中的每一行是一个json表达式。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scala">scala&gt; <span class="hljs-keyword">val</span> df = spark.read.json(<span class="hljs-string">&quot;data.json&quot;</span>)<br>df: org.apache.spark.sql.<span class="hljs-type">DataFrame</span> = [age: bigint, username: string]<br></code></pre></td></tr></table></figure>

<p>展示效果如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs scala">scala&gt; df.show()<br>+---+--------+<br>|age|username|<br>+---+--------+<br>| <span class="hljs-number">20</span>|zhangsan|<br>| <span class="hljs-number">30</span>|    lisi|<br>| <span class="hljs-number">40</span>|  wangwu|<br>+---+--------+<br></code></pre></td></tr></table></figure>

<blockquote>
<p>注意：如果是从内存中获取数据，Spark可以知道具体的数据类型是什么。如果是数字，则默认作为Int处理。如果是从文件中读取的数字，不能确定是什么类型，则使用bigint接收，可以与Long类型进行转换</p>
</blockquote>
<h2 id="使用SQL进行查询"><a href="#使用SQL进行查询" class="headerlink" title="使用SQL进行查询"></a>使用SQL进行查询</h2><p>我们可以在SparkSQL中直接使用SQL进行查询，但是这种风格的查询必须要有视图来进行辅助。视图分为全局视图和临时视图。</p>
<p>临时视图创建：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scala">scala&gt; df.createOrReplaceTempView(<span class="hljs-string">&quot;people&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>使用SQL进行查询：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs scala">scala&gt; <span class="hljs-keyword">val</span> sqlDF = spark.sql(<span class="hljs-string">&quot;select * from people&quot;</span>)<br>sqlDF: org.apache.spark.sql.<span class="hljs-type">DataFrame</span> = [age: bigint, username: string]<br><br>scala&gt; sqlDF.show()<br>+---+--------+<br>|age|username|<br>+---+--------+<br>| <span class="hljs-number">20</span>|zhangsan|<br>| <span class="hljs-number">30</span>|    lisi|<br>| <span class="hljs-number">40</span>|  wangwu|<br>+---+--------+<br></code></pre></td></tr></table></figure>

<p>全局视图创建：</p>
<p>普通临时表是Session范围内的，如果想要在应用范围内有效，应该使用全局的临时表。在访问的时候，也应该使用全路径进行访问。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs scala">scala&gt; df.createGlobalTempView(<span class="hljs-string">&quot;people1&quot;</span>)<br><br>scala&gt; spark.sql(<span class="hljs-string">&quot;select * from global_temp.people1&quot;</span>).show()<br>+---+--------+<br>|age|username|<br>+---+--------+<br>| <span class="hljs-number">20</span>|zhangsan|<br>| <span class="hljs-number">30</span>|    lisi|<br>| <span class="hljs-number">40</span>|  wangwu|<br>+---+--------+<br><br>scala&gt; spark.newSession().sql(<span class="hljs-string">&quot;select * from global_temp.people1&quot;</span>).show()<br>+---+--------+<br>|age|username|<br>+---+--------+<br>| <span class="hljs-number">20</span>|zhangsan|<br>| <span class="hljs-number">30</span>|    lisi|<br>| <span class="hljs-number">40</span>|  wangwu|<br>+---+--------+<br></code></pre></td></tr></table></figure>

<h2 id="使用DSL进行查询"><a href="#使用DSL进行查询" class="headerlink" title="使用DSL进行查询"></a>使用DSL进行查询</h2><p>DataFrame中还提供一个领域特定语言（Domain-Specific Language，DSL）来管理结构化的数据，可以在Scala、Java、Python和R中使用DSL。使用DSL则没有必要创建临时视图了。</p>
<p>查看DataFrame的schema信息：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs scala">scala&gt; df.printSchema<br>root<br> |-- age: long (nullable = <span class="hljs-literal">true</span>)<br> |-- username: string (nullable = <span class="hljs-literal">true</span>)<br></code></pre></td></tr></table></figure>

<p>查看某一列：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs scala">scala&gt; df.select(<span class="hljs-string">&quot;username&quot;</span>).show()<br>+--------+<br>|username|<br>+--------+<br>|zhangsan|<br>|    lisi|<br>|  wangwu|<br>+--------+<br></code></pre></td></tr></table></figure>

<p>对列进行运算：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs scala">scala&gt; df.select($<span class="hljs-string">&quot;username&quot;</span>, $<span class="hljs-string">&quot;age&quot;</span> + <span class="hljs-number">1</span>).show()<br>+--------+---------+<br>|username|(age + <span class="hljs-number">1</span>)|<br>+--------+---------+<br>|zhangsan|       <span class="hljs-number">21</span>|<br>|    lisi|       <span class="hljs-number">31</span>|<br>|  wangwu|       <span class="hljs-number">41</span>|<br>+--------+---------+<br><br>scala&gt; df.select(&#x27;username, &#x27;age + <span class="hljs-number">2</span>).show()<br>+--------+---------+<br>|username|(age + <span class="hljs-number">2</span>)|<br>+--------+---------+<br>|zhangsan|       <span class="hljs-number">22</span>|<br>|    lisi|       <span class="hljs-number">32</span>|<br>|  wangwu|       <span class="hljs-number">42</span>|<br>+--------+---------+<br></code></pre></td></tr></table></figure>

<blockquote>
<p>注意：涉及到对列进行运算的时候，每一列都必须使用$，或者采用引号表达式：<code>单引号+字段</code></p>
</blockquote>
<p>还有其他更多的操作，如filter，groupby等。</p>
<h2 id="创建DataSet"><a href="#创建DataSet" class="headerlink" title="创建DataSet"></a>创建DataSet</h2><p>DataSet的创建可以使用样例类序列创建，也可以使用基本类型序列创建</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs scala">scala&gt; <span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span>(<span class="hljs-params">name: <span class="hljs-type">String</span>, age: <span class="hljs-type">Long</span></span>)</span><br>defined <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span></span><br><br>scala&gt; <span class="hljs-keyword">val</span> caseClassDS = <span class="hljs-type">Seq</span>(<span class="hljs-type">Person</span>(<span class="hljs-string">&quot;zhangsan&quot;</span>, <span class="hljs-number">300</span>), <span class="hljs-type">Person</span>(<span class="hljs-string">&quot;lisi&quot;</span>, <span class="hljs-number">400</span>)).toDS<br>caseClassDS: org.apache.spark.sql.<span class="hljs-type">Dataset</span>[<span class="hljs-type">Person</span>] = [name: string, age: bigint]<br><br>scala&gt; caseClassDS.show<br>+--------+---+<br>|    name|age|<br>+--------+---+<br>|zhangsan|<span class="hljs-number">300</span>|<br>|    lisi|<span class="hljs-number">400</span>|<br>+--------+---+<br></code></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs scala">scala&gt; <span class="hljs-keyword">val</span> seqDS = <span class="hljs-type">Seq</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>).toDS<br>seqDS: org.apache.spark.sql.<span class="hljs-type">Dataset</span>[<span class="hljs-type">Int</span>] = [value: int]<br><br>scala&gt; seqDS.show<br>+-----+<br>|value|<br>+-----+<br>|    <span class="hljs-number">1</span>|<br>|    <span class="hljs-number">2</span>|<br>|    <span class="hljs-number">3</span>|<br>|    <span class="hljs-number">4</span>|<br>|    <span class="hljs-number">5</span>|<br>+-----+<br></code></pre></td></tr></table></figure>



<h1 id="三种抽象之间的关系"><a href="#三种抽象之间的关系" class="headerlink" title="三种抽象之间的关系"></a>三种抽象之间的关系</h1><p>到目前为止，Spark中为我们提供了三种抽象，分别是RDD，DataFrame，DataSet。它们之间有相似之处，也有不同点。</p>
<p>相同点：</p>
<ol>
<li>都是Spark平台下的分布式弹性数据集，为处理大型数据提供便利</li>
<li>都具有惰性机制，直到行动算子才会触发执行</li>
<li>会根据Spark的内存情况进行自动缓存</li>
<li>都有分区的概念</li>
<li>DataFrame和DataSet可以使用模式匹配来获取各个字段的值和类型</li>
</ol>
<p>不同点：</p>
<ol>
<li>RDD一般和Spark MLlib同时使用，不支持SparkSQL操作</li>
<li>DataFrame每一行的类型固定为Row，支持SparkSQL操作</li>
<li>DataSet和DataFrame具有完全相同的成员函数，区别在于每一行的数据类型不同</li>
</ol>
<p>它们分别是不同程度的抽象，可以互相转换。</p>
<blockquote>
<p>如果需要它们之间的相互操作，需要引入<code>import spark.implicits._</code>。这里的spark不是Scala的包名，而是创建的SparkSession对象的变量名称，所以需要创建SparkSession对象之后再导入。在spark-shell中无需我们手动导入，已经存在。</p>
<p>注意这里的spark对象不能使用var声明，因为Scala支持val修饰的对象进行导入。</p>
</blockquote>
<h2 id="RDD与-DataFrame之间的转换"><a href="#RDD与-DataFrame之间的转换" class="headerlink" title="RDD与 DataFrame之间的转换"></a>RDD与 DataFrame之间的转换</h2><p>RDD -&gt; DataFrame：<code>toDF</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs scala">scala&gt; <span class="hljs-keyword">val</span> rdd = sc.makeRDD(<span class="hljs-type">List</span>((<span class="hljs-string">&quot;zhangsan&quot;</span>, <span class="hljs-number">30</span>), (<span class="hljs-string">&quot;lisi&quot;</span>, <span class="hljs-number">40</span>)))<br>rdd: org.apache.spark.rdd.<span class="hljs-type">RDD</span>[(<span class="hljs-type">String</span>, <span class="hljs-type">Int</span>)] = <span class="hljs-type">ParallelCollectionRDD</span>[<span class="hljs-number">35</span>] at makeRDD at &lt;console&gt;:<span class="hljs-number">24</span><br><br>scala&gt; rdd.toDF.show()<br>+--------+---+<br>|      _1| _2|<br>+--------+---+<br>|zhangsan| <span class="hljs-number">30</span>|<br>|    lisi| <span class="hljs-number">40</span>|<br>+--------+---+<br><br>scala&gt; rdd.toDF(<span class="hljs-string">&quot;username&quot;</span>, <span class="hljs-string">&quot;age&quot;</span>).show()<br>+--------+---+<br>|username|age|<br>+--------+---+<br>|zhangsan| <span class="hljs-number">30</span>|<br>|    lisi| <span class="hljs-number">40</span>|<br>+--------+---+<br></code></pre></td></tr></table></figure>

<p>DataFrame -&gt; RDD：<code>.rdd</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scala">scala&gt; df.rdd.collect<br>res22: <span class="hljs-type">Array</span>[org.apache.spark.sql.<span class="hljs-type">Row</span>] = <span class="hljs-type">Array</span>([<span class="hljs-number">20</span>,zhangsan], [<span class="hljs-number">30</span>,lisi], [<span class="hljs-number">40</span>,wangwu])<br></code></pre></td></tr></table></figure>

<h2 id="DataFrame和DataSet之间的转换"><a href="#DataFrame和DataSet之间的转换" class="headerlink" title="DataFrame和DataSet之间的转换"></a>DataFrame和DataSet之间的转换</h2><p>DataFrame -&gt; DataSet：<code>as</code></p>
<p>注意需要属性和样例类属性名称保持一致</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs scala">scala&gt; <span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span>(<span class="hljs-params">name: <span class="hljs-type">String</span>, age: <span class="hljs-type">Long</span></span>)</span><br>defined <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span></span><br><br>scala&gt; <span class="hljs-keyword">val</span> df = sc.makeRDD(<span class="hljs-type">List</span>((<span class="hljs-string">&quot;zhangsan&quot;</span>, <span class="hljs-number">12</span>), (<span class="hljs-string">&quot;lisi&quot;</span>, <span class="hljs-number">13</span>))).toDF(<span class="hljs-string">&quot;name&quot;</span>, <span class="hljs-string">&quot;age&quot;</span>)<br>df: org.apache.spark.sql.<span class="hljs-type">DataFrame</span> = [name: string, age: int]<br><br>scala&gt; <span class="hljs-keyword">val</span> ds = df.as[<span class="hljs-type">Person</span>]<br>ds: org.apache.spark.sql.<span class="hljs-type">Dataset</span>[<span class="hljs-type">Person</span>] = [name: string, age: int<br></code></pre></td></tr></table></figure>

<p>DataSet -&gt; DataFrame：<code>toDF</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scala">scala&gt; ds.toDF<br>res32: org.apache.spark.sql.<span class="hljs-type">DataFrame</span> = [name: string, age: int]<br></code></pre></td></tr></table></figure>

<h2 id="RDD与DataSet之间的转换"><a href="#RDD与DataSet之间的转换" class="headerlink" title="RDD与DataSet之间的转换"></a>RDD与DataSet之间的转换</h2><p>RDD -&gt; DataSet：<code>toDS</code>。</p>
<p>SparkSQL能够自动将包含有case类的RDD转换为DataSet。样例类定义的表的结构，样例类的属性通过反射变为表的列名。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs scala">scala&gt; <span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span>(<span class="hljs-params">name: <span class="hljs-type">String</span>, age: <span class="hljs-type">Long</span></span>)</span><br>defined <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span></span><br><br>scala&gt; <span class="hljs-keyword">val</span> dataset = sc.makeRDD(<span class="hljs-type">List</span>(<span class="hljs-type">Person</span>(<span class="hljs-string">&quot;zhangsan&quot;</span>, <span class="hljs-number">11</span>), <span class="hljs-type">Person</span>(<span class="hljs-string">&quot;lisi&quot;</span>, <span class="hljs-number">22</span>))).toDS<br>dataset: org.apache.spark.sql.<span class="hljs-type">Dataset</span>[<span class="hljs-type">Person</span>] = [name: string, age: bigint]<br><br>scala&gt; dataset.show<br>+--------+---+<br>|    name|age|<br>+--------+---+<br>|zhangsan| <span class="hljs-number">11</span>|<br>|    lisi| <span class="hljs-number">22</span>|<br>+--------+---+<br></code></pre></td></tr></table></figure>

<p>DataSet -&gt; DataFrame：<code>.rdd</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scala">scala&gt; dataset.rdd.collect<br>res29: <span class="hljs-type">Array</span>[<span class="hljs-type">Person</span>] = <span class="hljs-type">Array</span>(<span class="hljs-type">Person</span>(zhangsan,<span class="hljs-number">11</span>), <span class="hljs-type">Person</span>(lisi,<span class="hljs-number">22</span>))<br></code></pre></td></tr></table></figure>

<h1 id="IDEA编程"><a href="#IDEA编程" class="headerlink" title="IDEA编程"></a>IDEA编程</h1><p>使用IDEA进行SparkSQL进行编程的话，需要进行依赖的添加：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.spark<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spark-sql_2.12<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>3.1.3<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br></code></pre></td></tr></table></figure>

<p>简单准备数据环境：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">SQL</span> </span>&#123;<br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span></span>(args: <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]): <span class="hljs-type">Unit</span> = &#123;<br>    <span class="hljs-comment">// 创建SparkSQL的运行环境</span><br>    <span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>).setAppName(<span class="hljs-string">&quot;sparkSQL&quot;</span>)<br>    <span class="hljs-keyword">val</span> spark = <span class="hljs-type">SparkSession</span>.builder().config(sparkConf).getOrCreate()<br>    <span class="hljs-keyword">import</span> spark.implicits._<br><br>    <span class="hljs-comment">// 创建RDD, DataFrame和DataSet</span><br>    <span class="hljs-keyword">val</span> rdd = spark.sparkContext.makeRDD(<span class="hljs-type">List</span>((<span class="hljs-number">1</span>, <span class="hljs-string">&quot;zhangsan&quot;</span>, <span class="hljs-number">30</span>), (<span class="hljs-number">2</span>, <span class="hljs-string">&quot;lisi&quot;</span>, <span class="hljs-number">40</span>)))<br>    <span class="hljs-keyword">val</span> df: <span class="hljs-type">DataFrame</span> = rdd.toDF(<span class="hljs-string">&quot;id&quot;</span>, <span class="hljs-string">&quot;name&quot;</span>, <span class="hljs-string">&quot;age&quot;</span>)<br>    <span class="hljs-keyword">val</span> ds:<span class="hljs-type">Dataset</span>[<span class="hljs-type">Person</span>] = df.as[<span class="hljs-type">Person</span>]<br><br>    df.show()<br>    ds.show()<br><br>    <span class="hljs-comment">// 环境关闭</span><br>    spark.close()<br>  &#125;<br><br>  <span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span>(<span class="hljs-params">id: <span class="hljs-type">Int</span>, name: <span class="hljs-type">String</span>, age: <span class="hljs-type">Int</span></span>)</span><br>&#125;<br></code></pre></td></tr></table></figure>

<h1 id="用户自定义函数"><a href="#用户自定义函数" class="headerlink" title="用户自定义函数"></a>用户自定义函数</h1><p>用户可以通过spark.udf功能添加自定义函数</p>
<h2 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h2><p>UDF函数只需要进行注册即可使用。例如这里的函数就是给姓名在查询的使用加一个前缀：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-comment">// 临时视图</span><br>ds.createOrReplaceTempView(<span class="hljs-string">&quot;person&quot;</span>)<br><br><span class="hljs-comment">// 自定义函数UDF</span><br>spark.udf.register(<span class="hljs-string">&quot;myUDF&quot;</span>, (name:<span class="hljs-type">String</span>) =&gt; &#123;<br>    <span class="hljs-string">&quot;Name: &quot;</span> + name<br>&#125;)<br>spark.sql(<span class="hljs-string">&quot;select myUDF(name), age from person&quot;</span>).show()<br></code></pre></td></tr></table></figure>

<h2 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a>UDAF</h2><p>UDAF指的是聚合操作。对于聚合操作来说，我们可以将它的执行过程分为两个步骤，首先进行遍历，将一些临时数据放在Buffer中，之后再对这些Buffer中的数据进行某种特殊的操作，得到聚合结果。</p>
<p>在SparkSQL中，实现UDAF也是类似的逻辑。我们需要继承特定的类，然后重写其中的方法，方法的总体逻辑与上面我们说的执行过程类似。这里可供继承的类有<code>UserDefinedAggregateFunction</code>和<code>Aggregator</code>，其中前者是弱类型的，需要使用顺序来确定值，而后者是强类型，可以使用类型名来确定值。前者已经不推荐使用，这里只做简单介绍。下面我们就使用用户自定义聚合函数来实现计算年龄的平均值</p>
<h3 id="UserDefinedAggregateFunction"><a href="#UserDefinedAggregateFunction" class="headerlink" title="UserDefinedAggregateFunction"></a>UserDefinedAggregateFunction</h3><p>自定义类继承<code>UserDefinedAggregateFunction</code>，实现其中的方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyAvg_UDAF</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">UserDefinedAggregateFunction</span> </span>&#123;<br>    <span class="hljs-comment">// 输入数据的结构</span><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">inputSchema</span></span>: <span class="hljs-type">StructType</span> = &#123;<br>        <span class="hljs-type">StructType</span>(<br>            <span class="hljs-type">Array</span>(<span class="hljs-type">StructField</span>(<span class="hljs-string">&quot;age&quot;</span>, <span class="hljs-type">LongType</span>))<br>        )<br>    &#125;<br><br>    <span class="hljs-comment">// buffer的结构</span><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bufferSchema</span></span>: <span class="hljs-type">StructType</span> = &#123;<br>        <span class="hljs-type">StructType</span>(<br>            <span class="hljs-type">Array</span>(<br>                <span class="hljs-type">StructField</span>(<span class="hljs-string">&quot;total&quot;</span>, <span class="hljs-type">LongType</span>),<br>                <span class="hljs-type">StructField</span>(<span class="hljs-string">&quot;count&quot;</span>, <span class="hljs-type">LongType</span>)<br>            )<br>        )<br>    &#125;<br><br>    <span class="hljs-comment">// 输出数据的数据类型</span><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dataType</span></span>: <span class="hljs-type">DataType</span> = <span class="hljs-type">LongType</span><br><br>    <span class="hljs-comment">// 函数稳定性</span><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">deterministic</span></span>: <span class="hljs-type">Boolean</span> = <span class="hljs-literal">true</span><br><br>    <span class="hljs-comment">// 缓冲区buffer初始化</span><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">initialize</span></span>(buffer: <span class="hljs-type">MutableAggregationBuffer</span>): <span class="hljs-type">Unit</span> = &#123;<br>        buffer.update(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>L)<br>        buffer.update(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>L)<br>    &#125;<br><br>    <span class="hljs-comment">// 根据输入的值更新buffer数据</span><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update</span></span>(buffer: <span class="hljs-type">MutableAggregationBuffer</span>, input: <span class="hljs-type">Row</span>): <span class="hljs-type">Unit</span> = &#123;<br>        buffer.update(<span class="hljs-number">0</span>, buffer.getLong(<span class="hljs-number">0</span>) + input.getLong(<span class="hljs-number">0</span>))<br>        buffer.update(<span class="hljs-number">1</span>, buffer.getLong(<span class="hljs-number">1</span>) + <span class="hljs-number">1</span>)<br>    &#125;<br><br>    <span class="hljs-comment">// 合并buffer数据,buffer2的数据合并到buffer1中</span><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">merge</span></span>(buffer1: <span class="hljs-type">MutableAggregationBuffer</span>, buffer2: <span class="hljs-type">Row</span>): <span class="hljs-type">Unit</span> = &#123;<br>        buffer1.update(<span class="hljs-number">0</span>, buffer1.getLong(<span class="hljs-number">0</span>) + buffer2.getLong(<span class="hljs-number">0</span>))<br>        buffer1.update(<span class="hljs-number">1</span>, buffer1.getLong(<span class="hljs-number">1</span>) + buffer2.getLong(<span class="hljs-number">1</span>))<br>    &#125;<br><br>    <span class="hljs-comment">// 计算聚合结果</span><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluate</span></span>(buffer: <span class="hljs-type">Row</span>): <span class="hljs-type">Any</span> = &#123;<br>        buffer.getLong(<span class="hljs-number">0</span>) / buffer.getLong(<span class="hljs-number">1</span>)<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<p>然后进行注册，在SQL语句中使用</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scala">spark.udf.register(<span class="hljs-string">&quot;myAvg_udaf&quot;</span>, <span class="hljs-keyword">new</span> <span class="hljs-type">MyAvg_UDAF</span>())<br>spark.sql(<span class="hljs-string">&quot;select myAvg_udaf(age) from person&quot;</span>).show()<br></code></pre></td></tr></table></figure>

<h3 id="Aggregator"><a href="#Aggregator" class="headerlink" title="Aggregator"></a>Aggregator</h3><p>可以看到上面的操作涉及到很多位置下标的使用，可读性不好也容易混淆。因此Spark提供了强类型的Aggregator，我们也需要自定义类来继承<code>Aggregator</code>，实现其中的方法。</p>
<p>注意这里不要导错对象了，正确路径为<code>import org.apache.spark.sql.expression.Aggregator</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-comment">// 需要指定泛型</span><br><span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Buffer</span>(<span class="hljs-params">var total: <span class="hljs-type">Long</span>, var count: <span class="hljs-type">Long</span></span>)</span><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyAvg_Agg</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Aggregator</span>[<span class="hljs-type">Long</span>, <span class="hljs-type">Buffer</span>, <span class="hljs-type">Long</span>] </span>&#123;<br>    <span class="hljs-comment">// 初始值，缓冲区初始化</span><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">zero</span></span>: <span class="hljs-type">Buffer</span> = &#123;<br>        <span class="hljs-type">Buffer</span>(<span class="hljs-number">0</span>L, <span class="hljs-number">0</span>L)<br>    &#125;<br><br>    <span class="hljs-comment">// 根据输入来更新缓冲区</span><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reduce</span></span>(buffer: <span class="hljs-type">Buffer</span>, in: <span class="hljs-type">Long</span>): <span class="hljs-type">Buffer</span> = &#123;<br>        buffer.total = buffer.total + in<br>        buffer.count = buffer.count + <span class="hljs-number">1</span><br>        buffer<br>    &#125;<br><br>    <span class="hljs-comment">// 合并缓冲区</span><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">merge</span></span>(buffer1: <span class="hljs-type">Buffer</span>, buffer2: <span class="hljs-type">Buffer</span>): <span class="hljs-type">Buffer</span> = &#123;<br>        buffer1.total = buffer1.total + buffer2.total<br>        buffer1.count = buffer1.count + buffer2.count<br>        buffer1<br>    &#125;<br><br>    <span class="hljs-comment">// 计算结果</span><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">finish</span></span>(buffer: <span class="hljs-type">Buffer</span>): <span class="hljs-type">Long</span> = &#123;<br>        buffer.total / buffer.count<br>    &#125;<br><br>    <span class="hljs-comment">// 缓冲区的编码操作(基本是固定格式，如果是自定义则为product，如果是系统自带则为scalaxxx)</span><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bufferEncoder</span></span>: <span class="hljs-type">Encoder</span>[<span class="hljs-type">Buffer</span>] = <span class="hljs-type">Encoders</span>.product<br><br>    <span class="hljs-comment">// 输出的编码操作</span><br>    <span class="hljs-keyword">override</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">outputEncoder</span></span>: <span class="hljs-type">Encoder</span>[<span class="hljs-type">Long</span>] = <span class="hljs-type">Encoders</span>.scalaLong<br>&#125;<br></code></pre></td></tr></table></figure>

<p>之后注册使用</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scala">spark.udf.register(<span class="hljs-string">&quot;myAvg_agg&quot;</span>, functions.udaf(<span class="hljs-keyword">new</span> <span class="hljs-type">MyAvg_Agg</span>()))<br>spark.sql(<span class="hljs-string">&quot;select myAvg_agg(age) from person&quot;</span>).show()<br></code></pre></td></tr></table></figure>

<h1 id="数据加载与保存"><a href="#数据加载与保存" class="headerlink" title="数据加载与保存"></a>数据加载与保存</h1><h2 id="通用方式"><a href="#通用方式" class="headerlink" title="通用方式"></a>通用方式</h2><p>SparkSQL提供通用方式进行数据的保存和加载，通用方式指的是使用相同的API，根据不同的参数读取和保存不同格式的数据。默认情况下，SparkSQL读取和保存的文件格式为parquet。（下面的spark都是SparkSession对象。）</p>
<p>通用加载：<code>spark.read.load</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scala">spark.read.format(<span class="hljs-string">&quot;...&quot;</span>)[.option(<span class="hljs-string">&quot;...&quot;</span>)].load(<span class="hljs-string">&quot;...&quot;</span>)<br></code></pre></td></tr></table></figure>

<ul>
<li>format：指定加载的数据格式，包括csv、jdbc、json、orc、parquet、textFile等</li>
<li>load：加载路径</li>
<li>option：传入相关参数</li>
</ul>
<p>通用保存：<code>df.write.save</code>(df是一个DataFrame对象)</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scala">df.write.format(<span class="hljs-string">&quot;...&quot;</span>)[.option(<span class="hljs-string">&quot;...&quot;</span>)].save(<span class="hljs-string">&quot;...&quot;</span>)<br></code></pre></td></tr></table></figure>

<ul>
<li>format：指定保存的数据格式</li>
<li>save：指定保存数据的路径</li>
<li>option：传入相关参数</li>
</ul>
<h2 id="不同格式"><a href="#不同格式" class="headerlink" title="不同格式"></a>不同格式</h2><p>上面是通过使用format来实现不同文件格式的加载，Spark中也提供专门的操作符来完成这件事：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs scala">scala&gt; spark.read.<br>csv format jdbc json load option options orc parquet schema <br>table text textFile<br></code></pre></td></tr></table></figure>

<ol>
<li>json：要求读取的json文件每一行是一个json串</li>
<li>csv：在option中配置csv文件的相关信息，如分隔符，是否包含表头等</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs scala">spark.read.format(<span class="hljs-string">&quot;csv&quot;</span>)<br>.option(<span class="hljs-string">&quot;sep&quot;</span>, <span class="hljs-string">&quot;;&quot;</span>)<br>.option(<span class="hljs-string">&quot;inferSchema&quot;</span>, <span class="hljs-string">&quot;true&quot;</span>)<br>.option(<span class="hljs-string">&quot;header&quot;</span>, <span class="hljs-string">&quot;true&quot;</span>)<br>.load(<span class="hljs-string">&quot;data/user.csv&quot;</span>)<br></code></pre></td></tr></table></figure>

<ol start="3">
<li>mysql：在IDEA中通过JDBC对MySQL进行操作</li>
</ol>
<p>先引入依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br> <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>mysql<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br> <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>mysql-connector-java<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br> <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>5.1.27<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br></code></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-comment">// 读取数据</span><br><span class="hljs-comment">//1.通用方式</span><br>spark.read.format(<span class="hljs-string">&quot;jdbc&quot;</span>)<br>.option(<span class="hljs-string">&quot;url&quot;</span>, <span class="hljs-string">&quot;jdbc:mysql://xxx&quot;</span>)<br>.option(<span class="hljs-string">&quot;driver&quot;</span>, <span class="hljs-string">&quot;com.mysql.jdbc.Driver&quot;</span>)<br>.option(<span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;xxx&quot;</span>)<br>.option(<span class="hljs-string">&quot;password&quot;</span>, <span class="hljs-string">&quot;xxx&quot;</span>)<br>.option(<span class="hljs-string">&quot;dbtable&quot;</span>, <span class="hljs-string">&quot;user_table&quot;</span>)<br>.load().show<br><br><span class="hljs-comment">//2.使用jdbc方法</span><br><span class="hljs-keyword">val</span> props: <span class="hljs-type">Properties</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">Properties</span>()<br>props.setProperty(<span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;xxx&quot;</span>)<br>props.setProperty(<span class="hljs-string">&quot;password&quot;</span>, <span class="hljs-string">&quot;xxxx&quot;</span>)<br><span class="hljs-keyword">val</span> df: <span class="hljs-type">DataFrame</span> = spark.read.jdbc(<span class="hljs-string">&quot;jdbc:mysql://xxx&quot;</span>, <span class="hljs-string">&quot;user_table&quot;</span>, props)<br>df.show<br></code></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-comment">// 保存数据</span><br><span class="hljs-comment">//1.通用方式</span><br>ds.write<br>.format(<span class="hljs-string">&quot;jdbc&quot;</span>)<br>.option(<span class="hljs-string">&quot;url&quot;</span>, <span class="hljs-string">&quot;jdbc:mysql://xxx&quot;</span>)<br>.option(<span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;xxx&quot;</span>)<br>.option(<span class="hljs-string">&quot;password&quot;</span>, <span class="hljs-string">&quot;xxx&quot;</span>)<br>.option(<span class="hljs-string">&quot;dbtable&quot;</span>, <span class="hljs-string">&quot;user_table&quot;</span>)<br>.mode(<span class="hljs-type">SaveMode</span>.<span class="hljs-type">Append</span>)<br>.save()<br><br><span class="hljs-comment">//2.通过jdbc方法</span><br><span class="hljs-keyword">val</span> props: <span class="hljs-type">Properties</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">Properties</span>()<br>props.setProperty(<span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;xxx&quot;</span>)<br>props.setProperty(<span class="hljs-string">&quot;password&quot;</span>, <span class="hljs-string">&quot;xxx&quot;</span>)<br>ds.write.mode(<span class="hljs-type">SaveMode</span>.<span class="hljs-type">Append</span>).jdbc(<span class="hljs-string">&quot;jdbc:mysql://xxx&quot;</span>, <span class="hljs-string">&quot;user_table&quot;</span>, props)<br><br></code></pre></td></tr></table></figure>

<ol start="4">
<li>Hive</li>
</ol>
<p>SparkSQL可以连接到部署好的Hive上，也可以使用自己的Hive元数据仓库。</p>
<p>如果需要连接外部Hive，应该将hive-site.xml复制到Spark的配置目录文件目录下<code>$SPARK_HOME/conf</code>。如果没有部署外部Hive，那么SparkSQL会在当前目录中创建自己的Hive元数据仓库，名称为<code>metastore_db</code>。</p>
<p>内嵌的Hive元数据存储在derby中，默认仓库地址为<code>$SPARK_HOME/spark-warehouse</code>，无需任何配置，直接使用即可。</p>
<p>外部的Hive连接需要以下操作：</p>
<ul>
<li>将<code>hive-site.xml</code>拷贝到<code>conf/</code>目录下</li>
<li>将MySQL的驱动复制到<code>jars/</code>目录下</li>
</ul>
<p>之后可以运行SparkSQL CLI，<code>bin/spark-sql</code>可以开启一个窗口，在其中直接执行SQL语句。</p>
<p>当然也可以运行Spark beeline。与HiveServer2类似，也是通过连接到MetaStore服务来进行元数据的访问。这需要我们启动Spark Thrift Server。它的接口和协议与HiveServer2完全一致，可以和Hive MetaStore进行交互。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sh">sbin/start-thriftserver.sh<br>bin/beeline -u jdbc:hive2://xxxx:10000 -n xxx<br></code></pre></td></tr></table></figure>

<p>也可以在代码中操作Hive，首先需要导入依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.spark<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spark-hive_2.12<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>3.0.0<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.hive<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>hive-exec<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>1.2.1<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>mysql<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>mysql-connector-java<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>5.1.27<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br></code></pre></td></tr></table></figure>

<p>然后将hive-site.xml文件拷贝到resources目录下。</p>
<p>创建SparkSession对象的时候，需要添加一个参数启动Hive支持</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-comment">//创建 SparkSession</span><br><span class="hljs-keyword">val</span> spark: <span class="hljs-type">SparkSession</span> = <span class="hljs-type">SparkSession</span><br> .builder()<br> .enableHiveSupport()<br> .master(<span class="hljs-string">&quot;local[*]&quot;</span>)<br> .appName(<span class="hljs-string">&quot;sql&quot;</span>)<br> .getOrCreate()<br></code></pre></td></tr></table></figure>

<blockquote>
<p>默认创建数据库在本地仓库，需要通过参数修改仓库地址</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scala">config(<span class="hljs-string">&quot;spark.sql.warehouse.dir&quot;</span>, <span class="hljs-string">&quot;hdfs://xxx/xxx/xx&quot;</span>)<br></code></pre></td></tr></table></figure>
</blockquote>

              
            </div>
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" class="category-chain-item">大数据</a>
  
  
    <span>></span>
    
  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/" class="category-chain-item">Spark</a>
  
  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E7%AC%94%E8%AE%B0/">#笔记</a>
      
        <a href="/tags/Spark/">#Spark</a>
      
        <a href="/tags/SparkSQL/">#SparkSQL</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Spark学习笔记-SparkSQL(1)-概述以及编程入门</div>
      <div>http://example.com/2022/07/26/Spark学习笔记-SparkSQL-1-概述以及编程入门/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>EverNorif</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2022年7月26日</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/07/26/Java%E9%9B%86%E5%90%88%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/" title="Java集合的基本使用">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Java集合的基本使用</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/07/21/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-SparkCore-4-Spark%E6%A0%B8%E5%BF%83%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6/" title="Spark学习笔记-SparkCore(4)-Spark核心运行机制">
                        <span class="hidden-mobile">Spark学习笔记-SparkCore(4)-Spark核心运行机制</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <i class="iconfont icon-love"></i> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>






  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.0/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script>
  (function() {
    var enableLang = CONFIG.code_language.enable && CONFIG.code_language.default;
    var enableCopy = CONFIG.copy_btn;
    if (!enableLang && !enableCopy) {
      return;
    }

    function getBgClass(ele) {
      return Fluid.utils.getBackgroundLightness(ele) >= 0 ? 'code-widget-light' : 'code-widget-dark';
    }

    var copyTmpl = '';
    copyTmpl += '<div class="code-widget">';
    copyTmpl += 'LANG';
    copyTmpl += '</div>';
    jQuery('.markdown-body pre').each(function() {
      var $pre = jQuery(this);
      if ($pre.find('code.mermaid').length > 0) {
        return;
      }
      if ($pre.find('span.line').length > 0) {
        return;
      }

      var lang = '';

      if (enableLang) {
        lang = CONFIG.code_language.default;
        if ($pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2 && $pre.children().hasClass('hljs')) {
          lang = $pre[0].children[0].classList[1];
        } else if ($pre[0].getAttribute('data-language')) {
          lang = $pre[0].getAttribute('data-language');
        } else if ($pre.parent().hasClass('sourceCode') && $pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2) {
          lang = $pre[0].children[0].classList[1];
          $pre.parent().addClass('code-wrapper');
        } else if ($pre.parent().hasClass('markdown-body') && $pre[0].classList.length === 0) {
          $pre.wrap('<div class="code-wrapper"></div>');
        }
        lang = lang.toUpperCase().replace('NONE', CONFIG.code_language.default);
      }
      $pre.append(copyTmpl.replace('LANG', lang).replace('code-widget">',
        getBgClass($pre[0]) + (enableCopy ? ' code-widget copy-btn" data-clipboard-snippet><i class="iconfont icon-copy"></i>' : ' code-widget">')));

      if (enableCopy) {
        Fluid.utils.createScript('https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js', function() {
          var clipboard = new window.ClipboardJS('.copy-btn', {
            target: function(trigger) {
              var nodes = trigger.parentNode.childNodes;
              for (var i = 0; i < nodes.length; i++) {
                if (nodes[i].tagName === 'CODE') {
                  return nodes[i];
                }
              }
            }
          });
          clipboard.on('success', function(e) {
            e.clearSelection();
            e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-copy', 'icon-success');
            setTimeout(function() {
              e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-success', 'icon-copy');
            }, 2000);
          });
        });
      }
    });
  })();
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/xiaomai.model.json"},"display":{"position":"left","width":150,"height":300,"vOffset":-90},"mobile":{"show":true},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
